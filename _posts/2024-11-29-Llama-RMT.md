---
layout: post
title: Analyzing Llama-3 weights via RMT
date: 2024-11-29 11:59:00-0400
description: Using random matrix theory to analyze Llama-3.2 weights
giscus_comments: false
related_posts: false
tags: deep_learning_tricks
---

## Prerequisite: Marchenko-Pastur (MP) Law 


### Definition

For $A \in \mathbb{R}^{m,n}$ with IID entries sampled with $\sigma \in \mathbb{R}^+$, the PDF of the singular values of $\frac{1}{m}AA^T$ is given by:

$$
\begin{aligned}
P_{\mathrm{MP}}(\nu) & = \begin{cases}\frac{n / m}{\pi \tilde{\sigma}^2 \nu} \sqrt{\left(\nu_{\max }^2-\nu^2\right)\left(\nu^2-\nu_{\min }^2\right)} & \nu \in\left[\nu_{\min }, \nu_{\max }\right] \\
0 & \text { else }\end{cases} \\
\nu_{\min } & =\tilde{\sigma}(1 \pm \sqrt{m / n}), \quad \tilde{\sigma}=\sigma \sqrt{n} .
\end{aligned}
$$

This PDF depends exclusively on the dimension of the matrix $(m,n)$ and the $\sigma$ that they're sampled from. 

### Plots for Kaiming initialized matrices

Kaiming initialization is a common initialization scheme used in neural networks. Functionally, it works by sampling each value of your weight matrix from $\mathcal{U}(-bound, bound)$, where $bound$ is calculated as:

$$bound = gain \times \sqrt{\frac{3}{\text{fan_mode}}}$$

In the below plots, I've plotted the MP distribution for a few matrix shapes (in red) versus the empirical distribution of singular values when the matrices are initialized with kaiming initialization:



<div class="equation">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="/assets/img/rmt/kaimingvstheoretical_128_3072.png" title="example image" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>


<div class="equation">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="/assets/img/rmt/kaimingvstheoretical_256_512.png" title="example image" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>


<div class="equation">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="/assets/img/rmt/kaimingvstheoretical_512_512.png" title="example image" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>



### Using deviation to MP as a qualitiative proxy for how close a matrix is to initialization

By comparing the histogram of a matrix's singular values to the MP distribution, we can qualitatively measure how far a matrix is from initialization by seeing how far the histogram is from the red curve. They match as we would expect.


## Analyzing Llama-3.2 with RMT

<div class="equation">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="/assets/img/rmt/llama_arc.png" title="example image" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

In this work, we'll anaylze the multi-head query, key and value matrices (`q_proj`, `k_proj`, `v_proj`).