<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="adityamehrotra.ca/feed.xml" rel="self" type="application/atom+xml"/><link href="adityamehrotra.ca/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-01-26T23:38:23+00:00</updated><id>adityamehrotra.ca/feed.xml</id><title type="html">blank</title><subtitle>Aditya Mehrotra&apos;s Personal Website </subtitle><entry><title type="html">An introduction to Slot Attention</title><link href="adityamehrotra.ca/blog/2024/Slot-Attention/" rel="alternate" type="text/html" title="An introduction to Slot Attention"/><published>2024-01-22T00:00:00+00:00</published><updated>2024-01-22T00:00:00+00:00</updated><id>adityamehrotra.ca/blog/2024/Slot-Attention</id><content type="html" xml:base="adityamehrotra.ca/blog/2024/Slot-Attention/"><![CDATA[<p>In this blog post, I’ll cover the basic slot attention mechanism and go over some intuition as to why it works.</p> <p>The problem we tackle is learning representations for particular regions of images, like a representation for a box, sphere or background of the scene.</p> <h2 id="intuition-k-means-clustering">Intuition, k-means clustering</h2> <h3 id="hard-k-means-clustering">Hard K-means clustering:</h3> <p>K-means clustering in the most simple form has the following steps:</p> <p><strong>Setup</strong>: Randomly initialize clusters by assigning each vector \(\mathbf{x}_n\) to one of \(N\) clusters</p> <p><strong>Repeat</strong>:</p> <ol> <li>Compute cluster means \(\boldsymbol{\mu}_n\) by averaging all points assigned to cluster \(n\)</li> <li>Reassign each each point to the cluster corresponding to it closest mean \(\boldsymbol{\mu}_k\).</li> <li>If some no new assignments were made, we’ve converged and can stop.</li> </ol> <p>While the example I gave above was for 2D Points, this algorithm has been used for images as well, for segmenting regions of the image by the pixel values in some colour space.</p> <p>When this algorithm converges, we expect to have \(N\) centroids, where each one should have a high affinity to the points closest to it.</p> <h3 id="soft-k-means-clustering">Soft K-Means clustering</h3> <p>The above algorithm doesn’t take into account that a certain point may share characteristics of more than one cluster. Hard K-Means is not expressive enough to capture this sort of relation, so we use Soft K-Means instead.</p> <p><strong>Setup</strong>: Randomly initialize clusters by assigning each vector \(\mathbf{x}_n\) to one of \(N\) clusters. We also define a distance measure \(\phi_n(i)\), that measures the “closeness” of \(\mathbf{x}_n\) to cluster \(\boldsymbol{\mu}_i\).</p> <p><strong>Repeat</strong>: Iterate the following</p> <ol> <li> <p>Updating \(\phi\):</p> \[\phi_n(i)=\frac{\exp \left\{-\frac{1}{\beta}\left\|\mathbf{x}_n-\boldsymbol{\mu}_i\right\|^2\right\}}{\sum_j \exp \left\{-\frac{1}{\beta}\left\|\mathbf{x}_n-\boldsymbol{\mu}_j\right\|^2\right\}}, \text { for } i=1, \ldots, N\] </li> <li> <p>Update \(\boldsymbol{\mu}_i\) : For each \(i\), update \(\boldsymbol{\mu}_k\) with the weighted average</p> </li> </ol> \[\boldsymbol{\mu}_i=\frac{\sum_n \mathbf{x}_n \phi_n(i)}{\sum_n \phi_n(i)}\] <p>Now we have a more expressive model, but notice a few things:</p> <ul> <li>The model is dependent on the data that it has fit to. Eg: Say you run this on image A and get some clusters. In order to recieve new clusters, you need to rerun the algorithm.</li> <li>Highly dependent on initialization. For some cluster initializations, you can get a very poor clustering performance</li> <li>The cluster mean might not be the best representation of the vectors inside the cluster itself</li> </ul> <p>This motivates the question, can we use parameters and highly expressive neural networks to perform much better than K-Means?</p> <h2 id="slot-attention">Slot Attention</h2> <h3 id="how-to-compute-slots">How to compute slots:</h3> <p>The slot attention operation works as follows:</p> <p><strong>Setup:</strong> Initialize embedding weights for key, query and value projection \(q( \cdot ), k(\cdot), v(\cdot)\). Also initialize \(N_\text{slots}\) slots of embedding dimension \(D\), basically a matrix of shape \(N_{\text{slots}} \times D\). These can be sampled from an isotropic normal with learned mean \(\boldsymbol{\mu} \in \mathbb{R}^{D}\)</p> <p><strong>Repeat T times</strong>:</p> <ol> <li>Given image of shape \(B\times 3 \times H \times W\), use a CNN encoder to encode this into a feature map of dimension \(B\times D \times H \times W\). Then flatten this into a sequence of tokens of shape \(N_{\text{data}} \times D\), call this \(\text{inputs}\).</li> <li> <p>Compute softmax weights over the data embeddings:</p> \[\text{Softmax}(\frac{1}{\sqrt{D}} k(\text{inputs}) q(\text{slots})^T, \text{axis='slots'})\] <p>Unpacking this, we have \(\text{inputs} \in \mathbb{R}^{N_{data} \times D}\) and \(\text{slots} \in \mathbb{R}^{N_{slots} \times D}\) (excluding batch dimension). Therefore, our softmax weights are of shape \((N_{\text{data}}, N_{\text{slots}})\) and the softmax is done across the \(N_{\text{slots}}\) axis. This is different from regular attention, which is done over the \(N_{\text{data}}\) axis, as this promotes “competition” across the slots. I’ll explain the intuition for that later.</p> </li> <li> <p>Compute slot updates as using a weighted average, the shape of which is \((N_{\text{slots}},D)\). Note that is the same shape as our slots.</p> \[\text{Softmax}(\frac{1}{\sqrt{D}} k(\text{inputs}) q(\text{slots})^T, \text{axis='slots'})^T v(\text{inputs})\] </li> <li>Update the previous slots using a small GRU network and the slot updates as the input.</li> </ol> <h3 id="intuition">Intuition</h3> <p>As you can see above, each slot update is a linear combination of \(\text{inputs}\). The coefficients of input embedding vector is determined by the softmax matrix, very similar to the regular attention mechanism we all know and love.</p> <p>To see why the slots enforce competition, we need to take a look at the softmax matrix in more detail. Denote \(I_i\) as the \(i^{th}\) input vector, and \(S_j\) as the \(j^{th}\) slot vector.</p> \[k(\text{inputs}) q(\text{slots})^T = \begin{pmatrix} I_1\cdot S_1 &amp; \cdots &amp; I_1 \cdot S_{N_{\text{slots}}} \\ \vdots &amp; \ddots &amp; \vdots \\ I_{N_{\text{data}}} \cdot S_1 &amp; \cdots &amp; I_{N_{\text{data}}}\cdot S_{N_{\text{slots}}} \end{pmatrix}\] <p>For which direction to take the softmax in, we have two options, either row wise (on the data axis) or column wise (on the slot axis):</p> <ul> <li> <p>If we normalize across the data axis, each row will sum up to one. So when we right multiply by \(v(\text{inputs})\), each slot update will be a convex linear combination of the input vectors. This is exactly what is used in the regular attention mechanism. However, each slot is unrestricted in what parts of the input sequence it can attend to. For example, the first embedding could have a softmax weight of \(1.0\).</p> </li> <li> <p>If we normalize across the slot dimension, each column will sum up to one. Now when we right multiply by \(v(\text{inputs})\), each slot update won’t be a convex linear combination anymore. However, now we are constraining the attention weights for each embedding across all slots. For example, if slot \(S\) has a high attention weight \(\approx 1\) for embedding \(I\), then it must be the case that the other slots have attention weights \(\approx 0\) for \(I\). This promotes “competition” for input vectors among the slots, as only a few slots will be able to have a high weight for any given input vector due to the properties of softmax.</p> </li> </ul> <p>Ultimately, if a slot has high coefficients for a set of input embedding vectors, it should be representative of those input embedding vectors.</p> <p>Therefore, I view this as a more expressive version of the K-Means operation we covered earlier, as the goal of both is to compute embeddings for distinct regions of the input images, and both do so via linear combinations of the input data. I believe expressive comes from the high degree of nonlinearity within the \(q,k,v\) projections and \(\text{Softmax}\).</p> <h3 id="what-does-this-actually-do">What does this actually do?</h3> <p>So now we run slot attention on a given image, and receive \(N_{\text{slots}}\) slots. What do we do now?</p> <p>Well, we need some sort of signal to update these weights, and quantify how good of a representation we learned. A simple answer (and what is used in the original paper) is to simply reconstruct the original image.</p> <p>So assume each slot has learned to have high affinity (high inner product) with a particular region of the image. Eg: One slot binded to a sphere, another to a cube. Then, if we reconstruct each slot into an image, we should get a set of images for each object the slot binds to.</p> <p>This is exactly what is done in the paper. Here is the sequence of steps to decode:</p> <ol> <li>For each slot of shape \((D, )\), use add two new spatial dimensions and repeat, getting a shape of \((D, \hat{H}, \hat{W})\)</li> <li>Run this through a series of transpose convolutions, to get images of shape \((4, H, W)\).</li> <li>The first 3 channels are RGB respectively, the last is an alpha channel. So for each slot \(i \in \{1, \dots, K\}\), we split each feature map into \(C_i \in \mathbb{R}^{3\times H \times W}\) and \(\alpha_i \in \mathbb{R}^{3 \times H \times W}\) (we simply took our alpha channel and repeated it 3 times, so the shape lines up with \(C_i\)).</li> <li>Finally we compose these by to get predicted image \(\hat{Y}\).</li> </ol> \[\hat{Y} = \sum_{i=1}^K \alpha_i \odot C_i\] <p>Our loss function is the simple MSE loss between original image \(Y\) and our predicted image \(\hat{Y}\):</p> \[L(\hat{Y}, Y) = \left | \left | \hat{Y} - Y \right | \right |^2_2\] <h3 id="visuals">Visuals:</h3> <p>Here’s a visual of the pipeline I presented above for this image with 7 objects and 7 slots. While the decoded image isn’t perfect, notice how each slot representation, when decoded, has roughly learned to represent a specific object in the scene.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://github.com/AditMeh/AditMeh.github.io/blob/master/assets/img/slot-attention/slot_attn_diagram?raw=true-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://github.com/AditMeh/AditMeh.github.io/blob/master/assets/img/slot-attention/slot_attn_diagram?raw=true-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://github.com/AditMeh/AditMeh.github.io/blob/master/assets/img/slot-attention/slot_attn_diagram?raw=true-1400.webp"/> <img src="https://github.com/AditMeh/AditMeh.github.io/blob/master/assets/img/slot-attention/slot_attn_diagram.png?raw=true" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="shortcomings-and-future-directions">Shortcomings and future directions:</h3> <p>To be added later…</p>]]></content><author><name></name></author><category term="computer_vision"/><summary type="html"><![CDATA[Going over the basics of Slot Attention, and covering some recent literature in the area]]></summary></entry><entry><title type="html">Inverse Transform Sampling in NeRF</title><link href="adityamehrotra.ca/blog/2023/NeRF-Inverse-Transform-Sampling/" rel="alternate" type="text/html" title="Inverse Transform Sampling in NeRF"/><published>2023-08-06T15:59:00+00:00</published><updated>2023-08-06T15:59:00+00:00</updated><id>adityamehrotra.ca/blog/2023/NeRF-Inverse-Transform-Sampling</id><content type="html" xml:base="adityamehrotra.ca/blog/2023/NeRF-Inverse-Transform-Sampling/"><![CDATA[<h2 id="problem-setup">Problem Setup</h2> <p>I was working on my implementation of NeRF and I read section 5.2 (Hierarchical Sampling). As discussed in the paper we have t-values \((t_1, \dots, t_{N_c})\), and we compute weights \(w_1, \dots, w_{N_c}\) as:</p> \[w_i=T_i\left(1-\exp \left(-\sigma_i \delta_i\right)\right)\] <p>They then normalize these weights to produce a piecewise constant PDF:</p> \[\hat{w}_i = \frac{w_i}{\sum_{j=1}^{N_c} w_j}\] <p>The authors propose using inverse transform sampling to sample t-values along the this ray around points that have a high \(\hat{w}_i\).</p> <p>When I read this my understanding of inverse transform sampling was limited to what’s described <a href="https://stephens999.github.io/fiveMinuteStats/inverse_transform_sampling.html#discrete_distributions">here</a>.</p> <p>However, none of these sampling techniques apply here because we don’t have the analytic form of an inverse CDF \(F^{-1}(x)\).</p> <p>Therefore, we need to construct this CDF and a method of evaluating it.</p> <h2 id="a-first-pass">A first pass</h2> <p>Let’s start simple, lets say we have the following ordered set of (t-value, \(\hat{w}\)) pairs \((t_1, \hat{w}_1), \dots, (t_{N_c}, \hat{w}_{N_c})\), such that \(\sum_{j=1}^{N_c} \hat{w}_j = 1\) and \(t_i &lt; t_{i+1}\).</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hierarchical/figure1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hierarchical/figure1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hierarchical/figure1-1400.webp"/> <img src="/assets/img/hierarchical/figure1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Okay, now let’s compute the cumulative sum. We will now have points \((t_1, \bar{w}_1), \dots, (t_{N_c}, \bar{w}_{N_c})\) where \(\bar{w}_i = \sum_{j=1}^{i} \hat{w}_j\) (note the last point should have a height of 1).</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hierarchical/figure2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hierarchical/figure2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hierarchical/figure2-1400.webp"/> <img src="/assets/img/hierarchical/figure2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Now, I want to somehow use this to perform inverse transform sampling, that is, mapping \(u \sim U[0,1]\) to a t-value \(t_u\).</p> <p>My first guess is to find the neighboring points on the cumulative sum function \((t_{left}, \bar{w}_{left}), (t_{right}, \bar{w}_{right})\) such that \(\bar{w}_{left} \leq t \leq \bar{w}_{right}\). With this, we can compute \(t_u\) as:</p> \[t_u = (t_{right} - t_{left}) \cdot r + t_{left}, \hspace{0.5cm} r= \frac{u - \bar{w}_{left}}{\bar{w}_{right} - \bar{w}_{left}}\] <p>Here are some visualizations of this procedure:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hierarchical/figure3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hierarchical/figure3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hierarchical/figure3-1400.webp"/> <img src="/assets/img/hierarchical/figure3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>However, notice what happens if \(u &lt; \bar{w}_1\), we cannot find a \(\bar{w}_{left}\) such that \(\bar{w}_{left} \leq u\). Therefore this method doesn’t work when \(u &lt; \bar{w}_1\).</p> <h2 id="an-online-solution">An online solution</h2> <p>I look around on the internet for how people implement this, and I found Krishna Murthy’s <a href="https://github.com/krrish94">NeRF Implementation</a>. The rest of the post will be going over how this is implemented.</p> <p>Krishna Murthy instead does the following:</p> <ol> <li>Take your t-values \((t_1, \dots, t_{N_c})\) and compute a new list of \(N_c-1\) points</li> </ol> \[\left (\frac{t_2 + t_1}{2}, \dots , \frac{t_{N_c} + t_{N_{c} - 1}}{2} \right )\] <ol> <li> <p>Take weights \((w_2, \dots, w_{N_c - 1})\). Note that they are not normalized. then replace each \(w_i\) with \(\hat{w}_i = \frac{w_i}{\sum_{j=2}^{N_c - 1} w_i}\). This is a list of \(N_c - 2\) probabilities. Append a 0 to the beginning of the list to get \((0, \hat{w}_2, \dots, \hat{w}_{N_c -1})\). Finally like before, replace each \(\hat{w}_i\) with \(\bar{w}_i = \sum_{j=2}^{i} \hat{w}_j\) to get \((0, \bar{w}_2, \dots, \bar{w}_{N_c -1})\). We now have \(N_c - 1\) probabilities.</p> </li> <li> <p>Pair up these \(N_c -1\) points and probabilities to get:</p> </li> </ol> \[\left(\left(\frac{t_{2} + t_{1}}{2}, 0 \right), \dots, \left(\frac{t_{N_c} + t_{N_{c} - 1}}{2}, \bar{w}_{N_c - 1} \right)\right)\] <p>Notice that the first point has a y value of 0, and the last has a y value of \(\bar{w}_{N_c-1} =1\) by definition. We now are able to find a left and right neighbor for all \(u \in [0,1]\), solving our previous problem.</p> <p>Let’s visualize what this function looks like compared to our previous one.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hierarchical/figure4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hierarchical/figure4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hierarchical/figure4-1400.webp"/> <img src="/assets/img/hierarchical/figure4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As you can see, it’s very similar to our previous attempt at a CDF and puts a lot of density around the regions with t-values that have a high probability. To illustrate this, I sampled 100 values \((u_1, ..., u_100)\) and computed \(t_{u_i}\) for each of them using the inverse transform sampling scheme I described earlier. On the left of the plot below, Each \(u_i\) is plotted in blue on the y-axis, and the corresponding \(t_{u_i}\)’s are also plotted in blue on the x axis.</p> <p>On the right of the plot below, I’ve plotted the original t-values and their normalized weights, exactly the same as the first figure in this blogpost. However, I’ve added the green dots on the x-axis which correspond to each of the \(t_{u_i}\)’s. As you can see, most of them lie in high density regions of the ray. This shows that with this method, we’re able to sample t-values from high density regions along the ray using inverse transform sampling.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hierarchical/figure5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hierarchical/figure5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hierarchical/figure5-1400.webp"/> <img src="/assets/img/hierarchical/figure5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="computer_vision"/><summary type="html"><![CDATA[Going over an implementation of inverse transform sampling in NeRF]]></summary></entry><entry><title type="html">Classifier-free diffusion guidance</title><link href="adityamehrotra.ca/blog/2023/diffusion_guidance/" rel="alternate" type="text/html" title="Classifier-free diffusion guidance"/><published>2023-03-05T15:59:00+00:00</published><updated>2023-03-05T15:59:00+00:00</updated><id>adityamehrotra.ca/blog/2023/diffusion_guidance</id><content type="html" xml:base="adityamehrotra.ca/blog/2023/diffusion_guidance/"><![CDATA[<p>In this post, I’ll describe a simple method on how to condition diffusion models called classifier-free guidance. I won’t go into the SDE side of things because I don’t understand it yet.</p> <h2 id="score-function">Score function</h2> <p>The score function for an arbitrary step along our markov chain \(\textbf{x}_t\) is defined as \(s_\theta(\textbf{x}_t, t) = \nabla_{\textbf{x}_t} \log q(\textbf{x}_t)\). Intuitively, this quantity tells us how to change the noisy \(\textbf{x}_t\) to make it more likely under the true data distribution.</p> <p>In diffusion models, we have a forward diffusion distribution \(q(\textbf{x}_t \mid \textbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha_t}} \textbf{x}_0, (1- \bar{\alpha_t}) \textbf{I})\). The gradient of the \(\log\) pdf of a gaussian \(\mathcal{N}(\textbf{x}; \mu, \sigma^2)\) can be computed as:</p> \[\begin{align*} &amp; \nabla_{\textbf{x}} p(\textbf{x}) = \nabla_\textbf{x} \left ( - \frac{1}{2 \sigma^2} (\textbf{x} - \mathbf{\mu})^2 \right) = -\frac{\textbf{x} - \mathbf{\mu}}{\sigma^2}\\ &amp;= -\frac{\mathbf{\epsilon}}{\sigma} \hspace{2 cm} (\textbf{x} = \mathbf{\mu} + \sigma \odot \mathbf{\epsilon}, \mathbf{\epsilon} \sim \mathcal{N}(\textbf{0, I})) \end{align*}\] <p>Therefore, we can express the score function of a sample \(\textbf{x}_t\) as:</p> \[\begin{align*} &amp;\mathbf{s}_\theta(\mathbf{x}_t, t) \approx \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t) = - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}} \end{align*}\] <p>I simply plugged the variance of \(q(\textbf{x}_t \mid \textbf{x}_0)\) and the diffusion model we train \(\mathbf{\epsilon}_{\theta} (\textbf{x}_t, t)\) is supposed to match the noise \(\mathbf{\epsilon}\), so I substitute that in as well.</p> <p>Therefore, I now have an equivalence of the score function in terms of our diffusion model’s U-Net. Learning the diffusion model as stated in DDPM is the same thing as learning the score function.</p> <h2 id="classifier-guidance">Classifier Guidance</h2> <p>Given our equivalence of the score function and noise prediction network, we can intuitively understand conditioning.</p> <p>If we have some auxillary input \(y\) that we want to condition on, we the need to model the score function \(\nabla_{\textbf{x}_t} \log q(\textbf{x}_t \mid \textbf{y})\). Hence, using bayes rule we can write this as:</p> \[\begin{align*} &amp; q(\textbf{x}_t \mid \textbf{y}) = \frac{q(\textbf{y} \mid \textbf{x}_t) q(\textbf{x}_t )}{q(\textbf{y})} \\ &amp; \implies \log q(\textbf{x}_t \mid \textbf{y}) = \log q(\textbf{y} \mid \textbf{x}_t) + \log q(\textbf{x}_t ) - \log q(\textbf{y}) \\ &amp; \implies \nabla_{\textbf{x}_t} \log q(\textbf{x}_t \mid \textbf{y}) = \nabla_{\textbf{x}_t} \log q(\textbf{y} \mid \textbf{x}_t) + \nabla_{\textbf{x}_t} \log q(\textbf{x}_t ) \end{align*}\] <p>It’s evident here that \(\nabla_{\textbf{x}_t} \log q(\textbf{y} \mid \textbf{x}_t)\) can be computed using a differentiable approximator, such as a softmax classifier (in the case of labels). We can add a hyperparameter \(s\) (called “guidance”), which controls how much influence this classifier has on our final prediction.</p> \[\nabla_{\textbf{x}_t} \log q(\textbf{x}_t \mid \textbf{y}) = \nabla_{\textbf{x}_t} \log q(\textbf{x}_t ) + s \cdot \nabla_{\textbf{x}_t} \log q(\textbf{y} \mid \textbf{x}_t)\] <p>The issue is, our \(\textbf{x}_t\) can be arbitrarily noisy and our classifier will not be able to be accurate at high levels of noise.</p> <h2 id="classifier-free-guidance">Classifier-Free Guidance</h2> <p>Hence, we seek to eliminate our dependence on a classifier, so we use bayes rule once again in the other direction:</p> \[\begin{align*} &amp; q(\textbf{y} \mid \textbf{x}_t) = \frac{q(\textbf{x}_t \mid \textbf{y}) q(\textbf{y})}{q(\textbf{x}_t)} \\ &amp; \implies \log q(\textbf{y} \mid \textbf{x}_t) = \log q(\textbf{x}_t \mid \textbf{y}) + \log q(\textbf{y}) - \log q(\textbf{x}_t) \\ &amp; \implies \nabla_{\textbf{x}_t} \log q(\textbf{y} \mid \textbf{x}_t) = \nabla_{\textbf{x}_t} \log q(\textbf{x}_t \mid \textbf{y}) - \nabla_{\textbf{x}_t} \log q(\textbf{x}_t) \\ \end{align*}\] <p>Plugging this back into our equation from Classifier Guidance:</p> \[\begin{align*} \nabla_{\textbf{x}_t} \log q(\textbf{x}_t \mid \textbf{y}) &amp;= \nabla_{\textbf{x}_t} \log q(\textbf{x}_t ) + s \cdot (\nabla_{\textbf{x}_t} \log q(\textbf{x}_t \mid \textbf{y}) - \nabla_{\textbf{x}_t} \log q(\textbf{x}_t)) \\ &amp;= (1-s) \cdot \nabla_{\textbf{x}_t} \log q(\textbf{x}_t ) + s \cdot \nabla_{\textbf{x}_t} \log q(\textbf{x}_t \mid \textbf{y}) \end{align*}\] <p>Ultimately, using our discussion of score functions earlier, we can equate this to learning a diffusion model as:</p> \[\begin{align*} \hat{\epsilon}(\textbf{x}_t, \textbf{y}, t) &amp;= -\frac{1}{\sqrt{1 - \bar{\alpha}_t}} \left ( (1-s) \cdot \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) + s \cdot \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, \mathbf{y}, t) \right) \end{align*}\] <p>We <em>could</em> just model \(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, \mathbf{y}, t)\) directly, however this formulation allows us to have more fine grained control over how much our learned conditional distribution affects the final generated sample.<br/> </p>]]></content><author><name></name></author><category term="generative_models"/><summary type="html"><![CDATA[A short derivation of classifier free diffusion guidance]]></summary></entry><entry><title type="html">Torch buffer vs Parameter</title><link href="adityamehrotra.ca/blog/2023/buffervsparameter/" rel="alternate" type="text/html" title="Torch buffer vs Parameter"/><published>2023-01-05T15:59:00+00:00</published><updated>2023-01-05T15:59:00+00:00</updated><id>adityamehrotra.ca/blog/2023/buffervsparameter</id><content type="html" xml:base="adityamehrotra.ca/blog/2023/buffervsparameter/"><![CDATA[<h1 id="torch-parameter-vs-buffer">Torch parameter vs buffer</h1> <h3 id="torch-parameters">Torch Parameters</h3> <p>You can create a parameter in your torch model by using the following in your init method.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)))</span></code></pre></figure> <p>When I save the model’s <code class="language-plaintext highlighter-rouge">state_dict</code>, I’ll find this within the <code class="language-plaintext highlighter-rouge">model.parameters()</code>. However, what if we had something that didn’t need gradient and hence did not need to be a parameter? An example would the mean and variance used in batch normalization. That’s where the next idea comes into play:</p> <h3 id="torch-buffers">Torch Buffers</h3> <p>You can create a buffer in your torch model by using the following in your init method of your <code class="language-plaintext highlighter-rouge">nn.Module</code></p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Usage: self.register_buffer(k, v)
</span><span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">"</span><span class="s">some_tensor</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span> </code></pre></figure> <p>In this example, <code class="language-plaintext highlighter-rouge">k</code> is a string <code class="language-plaintext highlighter-rouge">"some_tensor"</code> and <code class="language-plaintext highlighter-rouge">v</code> is a tensor of ones. I can now access this tensor via <code class="language-plaintext highlighter-rouge">self.some_tensor</code>, kind of like a python dictionary. The ones tensor never recieves and gradient, and will be stored in your state dict.</p> <h3 id="conclusion">Conclusion</h3> <p>So to wrap up, you should use parameters for things that require gradient and buffers for things that don’t. Of course, instead of buffers you can use <code class="language-plaintext highlighter-rouge">nn.Parameter</code> and set <code class="language-plaintext highlighter-rouge">requires_grad = False</code>, but your optimizer will need to check the <code class="language-plaintext highlighter-rouge">requires_grad</code> attribute of these tensors during the weight update, which is an unnecesary step.</p>]]></content><author><name></name></author><category term="programming"/><summary type="html"><![CDATA[A short writeup about the difference between torch buffers and parameters]]></summary></entry><entry><title type="html">Deriving the ELBO for VAEs</title><link href="adityamehrotra.ca/blog/2022/introduction-to-VAEs/" rel="alternate" type="text/html" title="Deriving the ELBO for VAEs"/><published>2022-11-15T15:59:00+00:00</published><updated>2022-11-15T15:59:00+00:00</updated><id>adityamehrotra.ca/blog/2022/introduction-to-VAEs</id><content type="html" xml:base="adityamehrotra.ca/blog/2022/introduction-to-VAEs/"><![CDATA[<p>Assume we have some log likelihood \(\log(p_\theta(x))\) we want to maximize, where the parameters of our probablistic model can be denoted as \(\theta\). Now, we express it in terms of joint probability of \(p_\theta(x, z)\) as:</p> \[\log \left ( \int_z p_\theta (x,z) dz \right )\] <p>We call \(z\) a “latent variable”. In the case that we have multiple latent variables in a vector \(\mathbf{z} \in \mathbb{R}^n\), we can write</p> \[\log \left ( \int_\mathbf{z} p_\theta (x,z_1, z_2, \dots, z_n) d\mathbf{z} \right )\] <p>This is intractable, as this requires way too many \(\mathbf{z}\) values to get a good enough approximation.</p> <p>Hence, if we lower bound this with something that IS tractable, then we’re able to optimize this.</p> <p>Since we have a joint probability \(p_\theta(x, \mathbf{z})\), we want to somehow decompose this into a product of two probabilities using the product rule. Below, you can see the graphical model, which tells us how we should be decomposing the joint distribution.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/elbo/VAE_graphical_model-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/elbo/VAE_graphical_model-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/elbo/VAE_graphical_model-1400.webp"/> <img src="/assets/img/elbo/VAE_graphical_model.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Graphical model of the VAE </div> <p>Therefore by decomposing the joint according to the graphical model:</p> \[= \log \left( \int_{\mathbf{z}} p_\theta (x \mid \mathbf{z}) p(\mathbf{z}) d\mathbf{z} \right )\] <p>We refer to \(p(\mathbf{z})\) as the “prior”, which is a distribution over our latent variables. Notice I don’t subscript this with \(\theta\), because it doesn’t have any paramters and is just a fixed distribution.</p> <p>Now, I will make a distinction. There are two distributions we are trying to learn: \(p(x \mid \mathbf{z})\) and \(p( \mathbf{z} \mid x)\). Overriding my previous notation, I denote \(\theta\) as the parameters for \(p_\theta(x \mid \mathbf{z})\) and \(\phi\) as the parameters for \(p_\phi(\mathbf{z} \mid x)\).</p> <p>Hence, going back to the log-likelihood, I multiply the insides by \(\frac{p_\phi (\mathbf{z} \mid x)}{p_\phi (\mathbf{z} \mid x)} = 1\).</p> \[= \log \left( \int_{\mathbf{z}} p_\theta (x \mid \mathbf{z}) p(\mathbf{z}) \cdot \frac{p_\phi (\mathbf{z} \mid x)}{p_\phi (\mathbf{z} \mid x)} d\mathbf{z} \right )\] <p>Then, I convert this into an expectation:</p> \[\log \left( \mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)}\left [ p_\theta (x \mid \mathbf{z}) \cdot \frac{p(\mathbf{z})}{p_\phi (\mathbf{z} \mid x)} \right ]\right )\] <p>At this point, we can apply jensen’s inequality, which tells us that \(\log \left( \mathbb{E}\left [X \right] \right ) \geq \mathbb{E}\left [\log(X) \right]\). Applying this gets us:</p> \[\log \left( \mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)}\left [ p_\theta (x \mid \mathbf{z}) \cdot \frac{p(\mathbf{z})}{p_\phi (\mathbf{z} \mid x)} \right ]\right ) \geq \mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)}\left [ \log \left( p_\theta (x \mid \mathbf{z}) \cdot \frac{p(\mathbf{z})}{p_\phi (\mathbf{z} \mid x)} \right ) \right ]\] <p>I will then simplify the right side:</p> \[\begin{align*} &amp;= \mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)} \left [ \log \left( p_\theta (x \mid \mathbf{z}) \right) + \log \left( p(\mathbf{z}) \right) - \log \left (p_\phi (\mathbf{z} \mid x) \right ) \right ] \\ &amp;=\mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)} \left [ \log \left( p_\theta (x \mid \mathbf{z}) \right) \right ] + \mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)} \left [ \log \left( p(\mathbf{z}) \right) \right ] - \mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)} \left [ \log \left (p_\phi (\mathbf{z} \mid x) \right ) \right ] \\ &amp;=\mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)} \left [ \log \left( p_\theta (x \mid \mathbf{z}) \right) \right ] - \text{KL} \left[ p(\mathbf{z})\mid \mid p_\phi (\mathbf{z} \mid x) \right ] \end{align*}\] <p>Where \(\text{KL}\) is the \(\text{KL}\) divergence between the prior \(p(\mathbf{z})\) and the posterior \(p_\phi(\mathbf{z} \mid x)\)</p>]]></content><author><name></name></author><category term="generative_models"/><summary type="html"><![CDATA[A simple derivation of the ELBO]]></summary></entry><entry><title type="html">Knowledge Distillation</title><link href="adityamehrotra.ca/blog/2022/knowledge_dist/" rel="alternate" type="text/html" title="Knowledge Distillation"/><published>2022-04-15T15:59:00+00:00</published><updated>2022-04-15T15:59:00+00:00</updated><id>adityamehrotra.ca/blog/2022/knowledge_dist</id><content type="html" xml:base="adityamehrotra.ca/blog/2022/knowledge_dist/"><![CDATA[<p>This post will go over the math and mechanics of how <a href="https://arxiv.org/abs/1503.02531">Knowledge Distillation</a> works and also include some code on how to implement it.</p> <h2> Preliminaries:</h2> <p>Say you have a classification task, where you have some feedforward net to classify the digits of MNIST. Denote the number of samples in the minibatch as \(m\), so we index a sample with \(i \in [1, m]\) and also denote the number of output classes as \(n \in \mathbb{N}\). The feedforward net produces unnormalized class scores (final output from the model), which we will denote as a vector \(z_i \in \mathbb{R}^n\). Finally, we retrieve a probability distribution over the output classes using the softmax function applied to the \(z_i\), we will denote this as \(a_i \in \mathbb{R}^n\). Finally, I will use an arbitrary \(k \in [0, n-1]\) to index my vectors. So \(z_{i,k}\) represents the \(k^{th}\) element of \(z_i\), and likewise for \(a_i\). Here is the formula:</p> \[\LARGE{a_{i,k} := \frac{e^{z_{i,k}}}{\sum_{j=0}^{n-1} e^{z_{i,j}}}}\] <p>Here is an example of it being used (rounded to four digits): </p> \[\begin{align*} \small{\begin{bmatrix} 2 \\ 10 \\ 3 \\ 0 \\ 5 \\ 4 \\ 7 \\ 9 \\ 1 \\ 2 \end{bmatrix}} \Longrightarrow softmax\left(\begin{bmatrix} 2 \\ 10 \\ 3 \\ 0 \\ 5 \\ 4 \\ 7 \\ 9 \\ 1 \\ 2 \end{bmatrix}\right) \Longrightarrow \begin{bmatrix} 0.0002 \\ 0.7000 \\ 0.0006 \\ 0.0000 \\ 0.0047 \\ 0.0017 \\ 0.0348 \\ 0.2575 \\ 0.0001 \\ 0.0002 \end{bmatrix} \end{align*}\] <p>The ground truth one hot vector is denoted $y_i$ for sample $i$ and the output softmax distribution is denote \(\hat{y_i} = a_i\). The loss function that is traditionally used is categorical cross entropy loss. Also, assume your ground truth labels are some one hot encoded vector, with a one at the index of the true class label. Here is its form with a few simplifications:</p> \[CE(y, \hat{y}) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{j=0}^{n-1} y^j \cdot log(\hat{y_i})= -\frac{1}{m} \sum_i^{m}log(\hat{y_i})\] <p>Intuitively, for each datapoint, we are trying to maximize the log probability of the GT class, disregarding the probabilities of the other classes.</p> <h2>Softmax Temperature:</h2> <p>Now, let us modify our softmax function a little bit:</p> \[\Large{a_{i,k} := \frac{e^{\frac{z_{i,k}}{T}}}{\displaystyle\sum_{j=1}^{n-1} e^{\frac{z_{i,j}}{T}}}}\] <p>I have introduced a new hyperparameter \(T\), which is commonly called “temperature” or “softmax temperature”.</p> <p>Notice that if \(T = 1\), we simply have our old softmax expression. Firstly, convince yourself that as \(T \rightarrow \infty\), each \(a_{i,k}\) will approach \(\frac{1}{n}\). This means that as \(T\) gets larger, the softmax distribution becomes a more softer probability distribution over the classes. Here are a few examples where \(n=5\) and \(z_{i} = [1,2,3,4,5]\):</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/distillation/temperature_example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/distillation/temperature_example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/distillation/temperature_example-1400.webp"/> <img src="/assets/img/distillation/temperature_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As you can see, the distribution gets softer and softer peaks as we increase \(T\) and generally seems to be approaching a uniform distribution. However, relationship between the class probabilities with regards to size stays about the same. As shown above, the classes 0-4 have increasing probability from right to left, except for very high values of \(T\).</p> <h2>"Dark Knowledge":</h2> <p>Now here’s the interesting bit, assume we trained a simple feedforward classifier on MNIST (\(n = 10\)). If we sample the following image and feed it into the classifier, we get the following softmax scores. \(T\) is set to 1, so this is just with the standard softmax function.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/distillation/regular_softmax_score-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/distillation/regular_softmax_score-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/distillation/regular_softmax_score-1400.webp"/> <img src="/assets/img/distillation/regular_softmax_score.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Pretty good right? Notice that the probability for the GT class <b>4</b> is much higher than the others, so our distribution peaks very highly at a certain point. The other probabilities are very small in comparison, and are not really interpertable. Now lets turn up the temperature to some higher values of \(T\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/distillation/various_temperatures-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/distillation/various_temperatures-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/distillation/various_temperatures-1400.webp"/> <img src="/assets/img/distillation/various_temperatures.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The distribution is getting softer and softer as we increase the temperature, and we begin to see the relationship between the smaller probabilities. The ground truth class is a <b>4</b>, but the image also looks a lot like a <b>9</b>. The classifier’s softmax distribution has a high probability given to <b>9</b> for temperature values of 5 and 10 compared to the other classes. This is what we mean by “dark knowledge”. By increasing the temperature, we reveal information about what other likely classes the image can belong to. We call these probability distributions generated through softmax with temperature “soft labels”. Soft labels are a lot more meaningful than the standard one hot encoded labels since they encode information about what classes the sample resembles. This boosts its ability to classify other classes that aren’t the ground truth class.</p> <h2>Aside: Why is there a meaningful relationship between probabilities of non-ground truth classes in a softmax distribution? </h2> <p>A core reason of why knowledge distillation works is because we assume that the softmax probabilities of classes that aren’t the ground truth class are meaningful. Specifically in the knowledge distillation paper, Hinton et al. state that:</p> <p><br/> <i> “Much of the information about the learned function resides in the ratios of very small probabilities in the soft targets. For example, one version of a 2 may be given a probability of \(10^6\) of being a 3 and \(10^9\) of being a 7 whereas for another version it may be the other way around. This is valuable information that defines a rich similarity structure over the data (i. e. it says which 2’s look like 3’s and which look like 7’s)” </i></p> <p>However, I wasn’t too sure what is the reasoning behind why we can say this and had the following question: <br/></p> <p><i>Couldn’t the model learn to give a high probability to the target class for an image and meaningless assorted probabilities to the others?</i></p> <p>After struggling with this question for a while, I found a satisfying answer from asking around on <a href="https://www.reddit.com/r/learnmachinelearning/comments/t5z17u/why_is_there_a_meaningful_relationship_between/">r/learnmachinelearning </a>. Here is the short version: First, we adopt the view that the model is learning to identify some <i>high-level features</i> from a given sample, where the final softmaxed scores say “How much do this sample’s features resemble what samples from class X would look like?”. Now, in datasets there is usually overlap between samples, even samples from different classes! Therefore, due to this overlap, it is evident that each of the class probabilities should contain information about how much the sample resembles class X, rather than the non-ground-truth probabilities being meaningless noise.</p> <h2>Distillation loss:</h2> <p>Given a model trained on a dataset using the standard softmax activation (when \(T=1\)), which we call a teacher model, we want to train another model (potentially with a different architecture) called the student model.</p> <p><br/></p> <p>Denote the logits of student network as \(z_i\) and denote the logits of the teacher network as \(\tilde z_i\). The student model is trained using the following objective, with a fixed hyperparameter \(T\):</p> \[L = \alpha \cdot CE(y_i, \text{S}(z_i)) +\\ (1- \alpha) \cdot CE(\text{S}(\frac{z_i}{T}), \text{S}(\frac{\tilde z_i}{T}))\] <p>Where \(S\) is the softmax function.</p> <p><br/> <br/> As you can see, we are trying to get the student model to maximize both the probability of the ground truth class through the first term. Additionally, we are also trying to match the distribution of the student softmax to the distribution of the teacher softmax, both with temperature \(T\). We can say this because of the connections between cross entropy and KL divergence.</p> <h2>Putting it together:</h2> <p>I will go over the steps required to distill knowledge from a teacher network into a (perhaps smaller) student network.</p> <ol> <li> First, train a teacher network as you would normally, the final activation on the logits needs to be a softmax. Save the weights.</li> <li> Train a student network, another classifier with a softmax activation, using the previously explained loss </li> </ol> <p><b>Thank you for reading this post!</b></p> <p>I’ve implemented all of the concepts I’ve talked about here, you can find my code <a href="https://github.com/AditMeh/Distillation">here</a>. I plan on updating this post with my experimental results (and some particularily interesting ones around learning unseen classes) at a later date.</p>]]></content><author><name></name></author><category term="deep_learning_tricks"/><category term="computer_vision"/><summary type="html"><![CDATA[Knowledge distillation]]></summary></entry></feed>