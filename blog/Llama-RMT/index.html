<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Analyzing Llama-3 weights via RMT | Aditya Mehrotra </title> <meta name="author" content="Aditya Mehrotra"> <meta name="description" content="Using random matrix theory to analyze Llama-3.2 weights"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%A6&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.cs.toronto.edu/~aditmeh//blog/Llama-RMT/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Aditya </span> Mehrotra </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Analyzing Llama-3 weights via RMT</h1> <p class="post-meta"> November 29, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/deep-learning-tricks"> <i class="fa-solid fa-hashtag fa-sm"></i> deep_learning_tricks</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="prerequisite-marchenko-pastur-mp-law">Prerequisite: Marchenko-Pastur (MP) Law</h2> <h3 id="definition">Definition</h3> <p>For \(A \in \mathbb{R}^{m,n}\) with IID entries sampled with \(\sigma \in \mathbb{R}^+\), the PDF of the singular values of \(\frac{1}{m}AA^T\) is given by:</p> \[\begin{aligned} P_{\mathrm{MP}}(\nu) &amp; = \begin{cases}\frac{n / m}{\pi \tilde{\sigma}^2 \nu} \sqrt{\left(\nu_{\max }^2-\nu^2\right)\left(\nu^2-\nu_{\min }^2\right)} &amp; \nu \in\left[\nu_{\min }, \nu_{\max }\right] \\ 0 &amp; \text { else }\end{cases} \\ \nu_{\min } &amp; =\tilde{\sigma}(1 \pm \sqrt{m / n}), \quad \tilde{\sigma}=\sigma \sqrt{n} . \end{aligned}\] <p>This PDF depends exclusively on the dimension of the matrix \((m,n)\) and the \(\sigma\) that they’re sampled from.</p> <h3 id="plots-for-kaiming-initialized-matrices">Plots for Kaiming initialized matrices</h3> <p>Kaiming initialization is a common initialization scheme used in neural networks. Functionally, it works by sampling each value of your weight matrix from \(\mathcal{U}(-bound, bound)\), where \(bound\) is calculated as:</p> \[bound = gain \times \sqrt{\frac{3}{\text{fan_mode}}}\] <p>In the below plots, I’ve plotted the MP distribution for a few matrix shapes (in red) versus the empirical distribution of singular values when the matrices are initialized with kaiming initialization:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/kaimingvstheoretical_128_3072-480.webp 480w,/assets/img/rmt/kaimingvstheoretical_128_3072-800.webp 800w,/assets/img/rmt/kaimingvstheoretical_128_3072-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/rmt/kaimingvstheoretical_128_3072.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/kaimingvstheoretical_256_512-480.webp 480w,/assets/img/rmt/kaimingvstheoretical_256_512-800.webp 800w,/assets/img/rmt/kaimingvstheoretical_256_512-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/rmt/kaimingvstheoretical_256_512.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/kaimingvstheoretical_512_512-480.webp 480w,/assets/img/rmt/kaimingvstheoretical_512_512-800.webp 800w,/assets/img/rmt/kaimingvstheoretical_512_512-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/rmt/kaimingvstheoretical_512_512.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="using-deviation-to-mp-as-a-qualitiative-proxy-for-how-close-a-matrix-is-to-initialization">Using deviation to MP as a qualitiative proxy for how close a matrix is to initialization</h3> <p>By comparing the histogram of a matrix’s singular values to the MP distribution, we can qualitatively measure how far a matrix is from initialization by seeing how far the histogram is from the red curve. They match as we would expect.</p> <h2 id="summary-of-locating-information-in-large-language-models-via-random-matrix-theory--by-staats-et-al">Summary of “Locating Information in Large Language Models via Random Matrix Theory “ by Staats et al.</h2> <p>This paper introduces the idea of analyzing BERT and Llama by qualitatively seeing differences in the MP distribution on averaged Llama 3 and BERT matrices.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/layer_avg-480.webp 480w,/assets/img/rmt/layer_avg-800.webp 800w,/assets/img/rmt/layer_avg-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/rmt/layer_avg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>As you can see above, we see that output projection matrices are closer to the MP distribution compared to queries on average.</p> <p>They also introduce a way of measuring the randomness of a matrix using a Kolmogorov-Smirnov test (explained later).</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/ks_paper-480.webp 480w,/assets/img/rmt/ks_paper-800.webp 800w,/assets/img/rmt/ks_paper-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/rmt/ks_paper.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The x-axis is the index of the singular vector for the given layer. The Y axis is the P-value. The higher the P-Value, the closer the singular vector is to being “random”. In thier paper, they use this to find “outlier” singular vectors, but in my case I’ve repurposed it as a measure of randomness of an entire matrix.</p> <h2 id="analyzing-llama-32-with-rmt">Analyzing Llama-3.2 with RMT</h2> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/llama_arc-480.webp 480w,/assets/img/rmt/llama_arc-800.webp 800w,/assets/img/rmt/llama_arc-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/rmt/llama_arc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>In this work, we’ll anaylze the multi-head query, key and value matrices (<code class="language-plaintext highlighter-rouge">q_proj</code>, <code class="language-plaintext highlighter-rouge">k_proj</code>, <code class="language-plaintext highlighter-rouge">v_proj</code>).</p> <h3 id="analyzing-the-query-matrix-across-heads-and-layers">Analyzing the query matrix across heads and layers</h3> <p>If we plot the query matrix for an early layer in the network, we get the following plot:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/query_layer=2-480.webp 480w,/assets/img/rmt/query_layer=2-800.webp 800w,/assets/img/rmt/query_layer=2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/rmt/query_layer=2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Recall the red curve is the MP distribution and the histogram is the distribution of singular values, so this means that the heads in this layer are very far from initialization (the MP distribution).</p> <p>Let’s repeat this for a later layer:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/Query_layer=27-480.webp 480w,/assets/img/rmt/Query_layer=27-800.webp 800w,/assets/img/rmt/Query_layer=27-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/rmt/Query_layer=27.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>As you can see, we have that the later layers are a lot closer to the theoretical MP distribution compared to the early ones.</p> <h3 id="quantitative-assessment-of-randomness-via-kolmogorovsmirnov-test">Quantitative assessment of randomness via Kolmogorov–Smirnov test</h3> <p>Given that the matrix has IID entries and finite variance, the entries of its singular vectors \(v\) of size \(n\) should follow a standard normal distribution with an std of \(\frac{1}{\sqrt{n}}\)</p> \[P\left(v_i\right)=\frac{1}{\sqrt{2 \pi / n}} \exp \left(-\frac{1}{2} v_i^2 n\right)\] <p>This lets us perform a Kolmogorov-Smirnov test via a theoretical gaussian CDF:</p> \[C_{\mathrm{G}}(x)=\frac{1}{2}+\frac{1}{2} \operatorname{erf}(\sqrt{n / 2} x)\] <p>In our case, I compute the P-Value for EACH singular vector in each head, and then I average those to get a single statistic for each head. I then average again to get a single statistic for each layer. Once this is done, we get the following plots:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/Query_KS-480.webp 480w,/assets/img/rmt/Query_KS-800.webp 800w,/assets/img/rmt/Query_KS-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/rmt/Query_KS.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/Key_KS-480.webp 480w,/assets/img/rmt/Key_KS-800.webp 800w,/assets/img/rmt/Key_KS-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/rmt/Key_KS.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>As you can see, the “randomness” of the weight matrices shoots up as a function of depth. Though I expected to see a linear relationship between layer and p-value, we can see instead that it saturates quite fast as we go deeper into the network.</p> <h2 id="conclusion">Conclusion</h2> <p>All I’ve really found evidence for is that the first few (1-5) layers of Llama 3.2 are <strong>very</strong> far from random init, and the later and middle layers of the network tend to be closer to random init.</p> <p>This method of showing such a phenomenon may be slighly exotic, but the results mentioned above aren’t new at all and have been found time and time again in the deep learning community.</p> <p>One example of this are papers that show you can chop off penultimate layers of a neural network and achieve minimal performance hits.</p> </div> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?8664456308d8a0b76907c75d01dd1dbf" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>