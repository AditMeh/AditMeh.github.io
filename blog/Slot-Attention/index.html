<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> An introduction to Slot Attention | Aditya Mehrotra </title> <meta name="author" content="Aditya Mehrotra"> <meta name="description" content="Going over the basics of Slot Attention, and covering some recent literature in the area"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%A6&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.cs.toronto.edu/~aditmeh//blog/Slot-Attention/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Aditya </span> Mehrotra </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">An introduction to Slot Attention</h1> <p class="post-meta"> January 22, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer_vision</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In this blog post, I’ll cover the basic slot attention mechanism <a class="citation" href="#locatello2020objectcentric">(Locatello et al., 2020)</a> and go over some intuition as to why it works.</p> <p>The problem we tackle is learning representations for particular regions of images, like a representation for a box, sphere or background of the scene.</p> <h2 id="intuition-k-means-clustering">Intuition, k-means clustering</h2> <h3 id="hard-k-means-clustering">Hard K-means clustering:</h3> <p>K-means clustering in the most simple form has the following steps:</p> <p><strong>Setup</strong>: Randomly initialize clusters by assigning each vector \(\mathbf{x}_n\) to one of \(N\) clusters</p> <p><strong>Repeat</strong>:</p> <ol> <li>Compute cluster means \(\boldsymbol{\mu}_n\) by averaging all points assigned to cluster \(n\)</li> <li>Reassign each each point to the cluster corresponding to it closest mean \(\boldsymbol{\mu}_k\).</li> <li>If some no new assignments were made, we’ve converged and can stop.</li> </ol> <p>While the example I gave above was for 2D Points, this algorithm has been used for images as well, for segmenting regions of the image by the pixel values in some colour space.</p> <p>When this algorithm converges, we expect to have \(N\) centroids, where each one should have a high affinity to the points closest to it.</p> <h3 id="soft-k-means-clustering">Soft K-Means clustering</h3> <p>The above algorithm doesn’t take into account that a certain point may share characteristics of more than one cluster. Hard K-Means is not expressive enough to capture this sort of relation, so we use Soft K-Means instead.</p> <p><strong>Setup</strong>: Randomly initialize clusters by assigning each vector \(\mathbf{x}_n\) to one of \(N\) clusters. We also define a distance measure \(\phi_n(i)\), that measures the “closeness” of \(\mathbf{x}_n\) to cluster \(\boldsymbol{\mu}_i\).</p> <p><strong>Repeat</strong>: Iterate the following</p> <ol> <li> <p>Updating \(\phi\):</p> \[\phi_n(i)=\frac{\exp \left\{-\frac{1}{\beta}\left\|\mathbf{x}_n-\boldsymbol{\mu}_i\right\|^2\right\}}{\sum_j \exp \left\{-\frac{1}{\beta}\left\|\mathbf{x}_n-\boldsymbol{\mu}_j\right\|^2\right\}}, \text { for } i=1, \ldots, N\] </li> <li> <p>Update \(\boldsymbol{\mu}_i\) : For each \(i\), update \(\boldsymbol{\mu}_k\) with the weighted average</p> </li> </ol> \[\boldsymbol{\mu}_i=\frac{\sum_n \mathbf{x}_n \phi_n(i)}{\sum_n \phi_n(i)}\] <p>Now we have a more expressive model, but notice a few things:</p> <ul> <li>The model is dependent on the data that it has fit to. Eg: Say you run this on image A and get some clusters. In order to recieve new clusters, you need to rerun the algorithm.</li> <li>Highly dependent on initialization. For some cluster initializations, you can get a very poor clustering performance</li> <li>The cluster mean might not be the best representation of the vectors inside the cluster itself</li> </ul> <p>This motivates the question, can we use parameters and highly expressive neural networks to perform much better than K-Means?</p> <h2 id="slot-attention">Slot Attention</h2> <h3 id="how-to-compute-slots">How to compute slots:</h3> <p>The slot attention operation works as follows:</p> <p><strong>Setup:</strong> Initialize embedding weights for key, query and value projection \(q( \cdot ), k(\cdot), v(\cdot)\) <a class="citation" href="#vaswani2023attention">(Vaswani et al., 2023)</a>. Also initialize \(N_\text{slots}\) slots of embedding dimension \(D\), basically a matrix of shape \(N_{\text{slots}} \times D\). These can be sampled from an isotropic normal with learned mean \(\boldsymbol{\mu} \in \mathbb{R}^{D}\)</p> <p><strong>Repeat T times</strong>:</p> <ol> <li>Given image of shape \(B\times 3 \times H \times W\), use a CNN encoder to encode this into a feature map of dimension \(B\times D \times H \times W\). Then flatten this into a sequence of tokens of shape \(N_{\text{data}} \times D\), call this \(\text{inputs}\).</li> <li> <p>Compute softmax weights over the data embeddings:</p> \[\text{Softmax}(\frac{1}{\sqrt{D}} k(\text{inputs}) q(\text{slots})^T, \text{axis='slots'})\] <p>Unpacking this, we have \(\text{inputs} \in \mathbb{R}^{N_{data} \times D}\) and \(\text{slots} \in \mathbb{R}^{N_{slots} \times D}\) (excluding batch dimension). Therefore, our softmax weights are of shape \((N_{\text{data}}, N_{\text{slots}})\) and the softmax is done across the \(N_{\text{slots}}\) axis. This is different from regular attention, which is done over the \(N_{\text{data}}\) axis, as this promotes “competition” across the slots. I’ll explain the intuition for that later.</p> </li> <li> <p>Compute slot updates as using a weighted average, the shape of which is \((N_{\text{slots}},D)\). Note that is the same shape as our slots.</p> \[\text{Softmax}(\frac{1}{\sqrt{D}} k(\text{inputs}) q(\text{slots})^T, \text{axis='slots'})^T v(\text{inputs})\] </li> <li>Update the previous slots using a small GRU network and the slot updates as the input.</li> </ol> <h3 id="intuition">Intuition</h3> <p>As you can see above, each slot update is a linear combination of \(\text{inputs}\). The coefficients of input embedding vector is determined by the softmax matrix, very similar to the regular attention mechanism we all know and love.</p> <p>To see why the slots enforce competition, we need to take a look at the softmax matrix in more detail. Denote \(I_i\) as the \(i^{th}\) input vector, and \(S_j\) as the \(j^{th}\) slot vector.</p> \[k(\text{inputs}) q(\text{slots})^T = \begin{pmatrix} I_1\cdot S_1 &amp; \cdots &amp; I_1 \cdot S_{N_{\text{slots}}} \\ \vdots &amp; \ddots &amp; \vdots \\ I_{N_{\text{data}}} \cdot S_1 &amp; \cdots &amp; I_{N_{\text{data}}}\cdot S_{N_{\text{slots}}} \end{pmatrix}\] <p>For which direction to take the softmax in, we have two options, either row wise (on the data axis) or column wise (on the slot axis):</p> <ul> <li> <p>If we normalize across the data axis, each row will sum up to one. So when we right multiply by \(v(\text{inputs})\), each slot update will be a convex linear combination of the input vectors. This is exactly what is used in the regular attention mechanism. However, each slot is unrestricted in what parts of the input sequence it can attend to. For example, the first embedding could have a softmax weight of \(1.0\).</p> </li> <li> <p>If we normalize across the slot dimension, each column will sum up to one. Now when we right multiply by \(v(\text{inputs})\), each slot update won’t be a convex linear combination anymore. However, now we are constraining the attention weights for each embedding across all slots. For example, if slot \(S\) has a high attention weight \(\approx 1\) for embedding \(I\), then it must be the case that the other slots have attention weights \(\approx 0\) for \(I\). This promotes “competition” for input vectors among the slots, as only a few slots will be able to have a high weight for any given input vector due to the properties of softmax.</p> </li> </ul> <p>Ultimately, if a slot has high coefficients for a set of input embedding vectors, it should be representative of those input embedding vectors.</p> <p>Therefore, I view this as a more expressive version of the K-Means operation we covered earlier, as the goal of both is to compute embeddings for distinct regions of the input images, and both do so via linear combinations of the input data. I believe expressive comes from the high degree of nonlinearity within the \(q,k,v\) projections and \(\text{Softmax}\).</p> <h3 id="what-does-this-actually-do">What does this actually do?</h3> <p>So now we run slot attention on a given image, and receive \(N_{\text{slots}}\) slots. What do we do now?</p> <p>Well, we need some sort of signal to update these weights, and quantify how good of a representation we learned. A simple answer (and what is used in the original paper) is to simply reconstruct the original image.</p> <p>So assume each slot has learned to have high affinity (high inner product) with a particular region of the image. Eg: One slot binded to a sphere, another to a cube. Then, if we reconstruct each slot into an image, we should get a set of images for each object the slot binds to.</p> <p>This is exactly what is done in the paper. Here is the sequence of steps to decode:</p> <ol> <li>For each slot of shape \((D, )\), use add two new spatial dimensions and repeat, getting a shape of \((D, \hat{H}, \hat{W})\)</li> <li>Run this through a series of transpose convolutions, to get images of shape \((4, H, W)\).</li> <li>The first 3 channels are RGB respectively, the last is an alpha channel. So for each slot \(i \in \{1, \dots, K\}\), we split each feature map into \(C_i \in \mathbb{R}^{3\times H \times W}\) and \(\alpha_i \in \mathbb{R}^{3 \times H \times W}\) (we simply took our alpha channel and repeated it 3 times, so the shape lines up with \(C_i\)).</li> <li>Finally we compose these by to get predicted image \(\hat{Y}\).</li> </ol> \[\hat{Y} = \sum_{i=1}^K \alpha_i \odot C_i\] <p>Our loss function is the simple MSE loss between original image \(Y\) and our predicted image \(\hat{Y}\):</p> \[L(\hat{Y}, Y) = \left | \left | \hat{Y} - Y \right | \right |^2_2\] <h3 id="visuals">Visuals:</h3> <p>Here’s a visual of the pipeline I presented above for this image with 7 objects and 7 slots. While the decoded image isn’t perfect, notice how each slot representation, when decoded, has roughly learned to represent a specific object in the scene.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://github.com/AditMeh/AditMeh.github.io/blob/master/assets/img/slot-attention/slot_attn_diagram?raw=true-480.webp 480w,https://github.com/AditMeh/AditMeh.github.io/blob/master/assets/img/slot-attention/slot_attn_diagram?raw=true-800.webp 800w,https://github.com/AditMeh/AditMeh.github.io/blob/master/assets/img/slot-attention/slot_attn_diagram?raw=true-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="https://github.com/AditMeh/AditMeh.github.io/blob/master/assets/img/slot-attention/slot_attn_diagram.png?raw=true" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="shortcomings-and-future-directions">Shortcomings and future directions:</h3> <p>To be added later…</p> </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="vaswani2023attention" class="col-sm-8"> <div class="title">Attention Is All You Need</div> <div class="author"> Ashish Vaswani ,  Noam Shazeer ,  Niki Parmar , and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="locatello2020objectcentric" class="col-sm-8"> <div class="title">Object-Centric Learning with Slot Attention</div> <div class="author"> Francesco Locatello ,  Dirk Weissenborn ,  Thomas Unterthiner , and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?8664456308d8a0b76907c75d01dd1dbf" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>