<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="adityamehrotra.ca/feed.xml" rel="self" type="application/atom+xml"/><link href="adityamehrotra.ca/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-20T21:19:41+00:00</updated><id>adityamehrotra.ca/feed.xml</id><title type="html">blank</title><subtitle>Aditya Mehrotra&apos;s Personal Website </subtitle><entry><title type="html">Analyzing Llama-3 weights via RMT</title><link href="adityamehrotra.ca/blog/Llama-RMT/" rel="alternate" type="text/html" title="Analyzing Llama-3 weights via RMT"/><published>2024-11-29T15:59:00+00:00</published><updated>2024-11-29T15:59:00+00:00</updated><id>adityamehrotra.ca/blog/Llama-RMT</id><content type="html" xml:base="adityamehrotra.ca/blog/Llama-RMT/"><![CDATA[<h2 id="prerequisite-marchenko-pastur-mp-law">Prerequisite: Marchenko-Pastur (MP) Law</h2> <h3 id="definition">Definition</h3> <p>For \(A \in \mathbb{R}^{m,n}\) with IID entries sampled with \(\sigma \in \mathbb{R}^+\), the PDF of the singular values of \(\frac{1}{m}AA^T\) is given by:</p> \[\begin{aligned} P_{\mathrm{MP}}(\nu) &amp; = \begin{cases}\frac{n / m}{\pi \tilde{\sigma}^2 \nu} \sqrt{\left(\nu_{\max }^2-\nu^2\right)\left(\nu^2-\nu_{\min }^2\right)} &amp; \nu \in\left[\nu_{\min }, \nu_{\max }\right] \\ 0 &amp; \text { else }\end{cases} \\ \nu_{\min } &amp; =\tilde{\sigma}(1 \pm \sqrt{m / n}), \quad \tilde{\sigma}=\sigma \sqrt{n} . \end{aligned}\] <p>This PDF depends exclusively on the dimension of the matrix \((m,n)\) and the \(\sigma\) that they’re sampled from.</p> <h3 id="plots-for-kaiming-initialized-matrices">Plots for Kaiming initialized matrices</h3> <p>Kaiming initialization is a common initialization scheme used in neural networks. Functionally, it works by sampling each value of your weight matrix from \(\mathcal{U}(-bound, bound)\), where \(bound\) is calculated as:</p> \[bound = gain \times \sqrt{\frac{3}{\text{fan_mode}}}\] <p>In the below plots, I’ve plotted the MP distribution for a few matrix shapes (in red) versus the empirical distribution of singular values when the matrices are initialized with kaiming initialization:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/kaimingvstheoretical_128_3072-480.webp 480w,/assets/img/rmt/kaimingvstheoretical_128_3072-800.webp 800w,/assets/img/rmt/kaimingvstheoretical_128_3072-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rmt/kaimingvstheoretical_128_3072.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/kaimingvstheoretical_256_512-480.webp 480w,/assets/img/rmt/kaimingvstheoretical_256_512-800.webp 800w,/assets/img/rmt/kaimingvstheoretical_256_512-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rmt/kaimingvstheoretical_256_512.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/kaimingvstheoretical_512_512-480.webp 480w,/assets/img/rmt/kaimingvstheoretical_512_512-800.webp 800w,/assets/img/rmt/kaimingvstheoretical_512_512-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rmt/kaimingvstheoretical_512_512.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="using-deviation-to-mp-as-a-qualitiative-proxy-for-how-close-a-matrix-is-to-initialization">Using deviation to MP as a qualitiative proxy for how close a matrix is to initialization</h3> <p>By comparing the histogram of a matrix’s singular values to the MP distribution, we can qualitatively measure how far a matrix is from initialization by seeing how far the histogram is from the red curve. They match as we would expect.</p> <h2 id="summary-of-locating-information-in-large-language-models-via-random-matrix-theory--by-staats-et-al">Summary of “Locating Information in Large Language Models via Random Matrix Theory “ by Staats et al.</h2> <p>This paper introduces the idea of analyzing BERT and Llama by qualitatively seeing differences in the MP distribution on averaged Llama 3 and BERT matrices.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/layer_avg-480.webp 480w,/assets/img/rmt/layer_avg-800.webp 800w,/assets/img/rmt/layer_avg-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rmt/layer_avg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As you can see above, we see that output projection matrices are closer to the MP distribution compared to queries on average.</p> <p>They also introduce a way of measuring the randomness of a matrix using a Kolmogorov-Smirnov test (explained later).</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/ks_paper-480.webp 480w,/assets/img/rmt/ks_paper-800.webp 800w,/assets/img/rmt/ks_paper-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rmt/ks_paper.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The x-axis is the index of the singular vector for the given layer. The Y axis is the P-value. The higher the P-Value, the closer the singular vector is to being “random”. In thier paper, they use this to find “outlier” singular vectors, but in my case I’ve repurposed it as a measure of randomness of an entire matrix.</p> <h2 id="analyzing-llama-32-with-rmt">Analyzing Llama-3.2 with RMT</h2> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/llama_arc-480.webp 480w,/assets/img/rmt/llama_arc-800.webp 800w,/assets/img/rmt/llama_arc-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rmt/llama_arc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In this work, we’ll anaylze the multi-head query, key and value matrices (<code class="language-plaintext highlighter-rouge">q_proj</code>, <code class="language-plaintext highlighter-rouge">k_proj</code>, <code class="language-plaintext highlighter-rouge">v_proj</code>).</p> <h3 id="analyzing-the-query-matrix-across-heads-and-layers">Analyzing the query matrix across heads and layers</h3> <p>If we plot the query matrix for an early layer in the network, we get the following plot:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/query_layer=2-480.webp 480w,/assets/img/rmt/query_layer=2-800.webp 800w,/assets/img/rmt/query_layer=2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rmt/query_layer=2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Recall the red curve is the MP distribution and the histogram is the distribution of singular values, so this means that the heads in this layer are very far from initialization (the MP distribution).</p> <p>Let’s repeat this for a later layer:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/Query_layer=27-480.webp 480w,/assets/img/rmt/Query_layer=27-800.webp 800w,/assets/img/rmt/Query_layer=27-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rmt/Query_layer=27.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As you can see, we have that the later layers are a lot closer to the theoretical MP distribution compared to the early ones.</p> <h3 id="quantitative-assessment-of-randomness-via-kolmogorovsmirnov-test">Quantitative assessment of randomness via Kolmogorov–Smirnov test</h3> <p>Given that the matrix has IID entries and finite variance, the entries of its singular vectors \(v\) of size \(n\) should follow a standard normal distribution with an std of \(\frac{1}{\sqrt{n}}\)</p> \[P\left(v_i\right)=\frac{1}{\sqrt{2 \pi / n}} \exp \left(-\frac{1}{2} v_i^2 n\right)\] <p>This lets us perform a Kolmogorov-Smirnov test via a theoretical gaussian CDF:</p> \[C_{\mathrm{G}}(x)=\frac{1}{2}+\frac{1}{2} \operatorname{erf}(\sqrt{n / 2} x)\] <p>In our case, I compute the P-Value for EACH singular vector in each head, and then I average those to get a single statistic for each head. I then average again to get a single statistic for each layer. Once this is done, we get the following plots:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/Query_KS-480.webp 480w,/assets/img/rmt/Query_KS-800.webp 800w,/assets/img/rmt/Query_KS-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rmt/Query_KS.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rmt/Key_KS-480.webp 480w,/assets/img/rmt/Key_KS-800.webp 800w,/assets/img/rmt/Key_KS-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/rmt/Key_KS.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As you can see, the “randomness” of the weight matrices shoots up as a function of depth. Though I expected to see a linear relationship between layer and p-value, we can see instead that it saturates quite fast as we go deeper into the network.</p> <h2 id="conclusion">Conclusion</h2> <p>All I’ve really found evidence for is that the first few (1-5) layers of Llama 3.2 are <strong>very</strong> far from random init, and the later and middle layers of the network tend to be closer to random init.</p> <p>This method of showing such a phenomenon may be slighly exotic, but the results mentioned above aren’t new at all and have been found time and time again in the deep learning community.</p> <p>One example of this are papers that show you can chop off penultimate layers of a neural network and achieve minimal performance hits.</p>]]></content><author><name></name></author><category term="deep_learning_tricks"/><summary type="html"><![CDATA[Using random matrix theory to analyze Llama-3.2 weights]]></summary></entry><entry><title type="html">Implicit gradients and Dirichlet uncertainty</title><link href="adityamehrotra.ca/blog/Dirichlet-Reparameterization/" rel="alternate" type="text/html" title="Implicit gradients and Dirichlet uncertainty"/><published>2024-10-26T15:59:00+00:00</published><updated>2024-10-26T15:59:00+00:00</updated><id>adityamehrotra.ca/blog/Dirichlet-Reparameterization</id><content type="html" xml:base="adityamehrotra.ca/blog/Dirichlet-Reparameterization/"><![CDATA[<p>Goal is to compute</p> \[\nabla_\theta \mathcal{L} = \nabla_\theta \mathbb{E}_{q_\theta(z)}[f_\theta(z)]\] <p>The transform of \(z \sim q_\theta(z)\) by the cdf \(F_\theta(z)\) is uniformly distributed.</p> <p>Let \(u \sim U(0,1)\) be a randomly sampled uniform value. Therefore, for some \(z \in \mathbb{R}\), we have that</p> \[u = F_\theta(x) = \int_{-\infty}^z q_\theta(z') dz'\] <p>Taking the derivative of both sides, we get that:</p> \[0 = \dfrac{\partial z}{\partial \theta} q_\theta(z) + \int_{-\infty}^z \dfrac{\partial}{\partial \theta } q_\theta(z')dz'\] <p>Hence,</p> \[\dfrac{\partial z}{\partial \theta} = \frac{-\dfrac{\partial}{\partial \theta} F_\theta(z)}{q_\theta(z)}\] <p>??? (I don’t understand how to fill in the steps here)</p> <p>Profit: \(\nabla_\theta \mathcal{L}=\mathbb{E}_{q_\theta(z)}\left[\frac{d f_\theta(z)}{d z} \frac{d z}{d \theta}+\frac{\partial f_\theta(z)}{\partial \theta}\right]\)</p>]]></content><author><name></name></author><category term="deep_learning_tricks"/><summary type="html"><![CDATA[Goal is to compute]]></summary></entry><entry><title type="html">Turning your diffusion model into a classifier</title><link href="adityamehrotra.ca/blog/Diffusion-Classifier/" rel="alternate" type="text/html" title="Turning your diffusion model into a classifier"/><published>2024-09-08T15:59:00+00:00</published><updated>2024-09-08T15:59:00+00:00</updated><id>adityamehrotra.ca/blog/Diffusion-Classifier</id><content type="html" xml:base="adityamehrotra.ca/blog/Diffusion-Classifier/"><![CDATA[<p>In this post, we’ll be going over the “Your Diffusion model is secretly a zero-shot classifier” paper (https://arxiv.org/abs/2303.16203).</p> <h2 id="diffusion-model-recap">Diffusion model recap</h2> <p>The main idea of a diffusion model is that we’re learning an approximation to the true data distribution called \(p_\theta(x_0)\) by maximimizing an ELBO:</p> \[p_\theta(x_0) \geq - \mathbb{E}_{t, \epsilon} \left [ || \epsilon - \epsilon_\theta (x_t)||^2 \right ]\] <p>If we want to do conditional diffusion, we simply can add a condition vector \(c\), and model the conditional</p> \[p_\theta(x_0 \mid c) \geq - \mathbb{E}_{t, \epsilon} \left [ || \epsilon - \epsilon_\theta (x_t, c)||^2 \right ]\] <h2 id="bayesian-flippy-flip">Bayesian flippy flip</h2> <p>Say we’ve learned \(p_\theta(x \mid c)\), how do we turn this into a classifier? Well, we’ll need to model \(p_\theta(c \mid x_0)\).</p> <p>Here is the key derivation:</p> \[p_\theta(c = c_i | x_0) = \frac{p(c_i)p_\theta(x_0 \mid c = c_i)}{\sum_j p(c = c_j)p_\theta(x_0 \mid c = c_j) }\] <p>However, since we were kids, we’ve been told that \(p_\theta(x_0 \mid c)\) is NOT tractable! Thankfully, ELBO (when tight enough) is basically equal to \(\log p_\theta (x_0 \mid c)\).</p> <p>Therefore, we can simply sub our ELBO into this and get:</p> \[p_\theta\left(c = c_i \mid x_0 \right)=\frac{\exp \left\{-\mathbb{E}_{t, \epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(x_t, c_i\right)\right\|^2\right]\right\}}{\sum_j \exp \left\{-\mathbb{E}_{t, \epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(x_t, c_j\right)\right\|^2\right]\right\}}\] <p>You can approximate the expectation by using monte carlo sampling given a datapoint \(x\) and conditioning inputs \(\{c_i \}_{i=1}^n\) using the following steps:</p> <p>Firstly, create a list \(\text{Error}[c_i] = 0\), for each \(c_i\).</p> <p>Then, for \(N\) trials,</p> <ol> <li> <p>Sample \(t \sim \text{Unif}[1, T]\), and \(\epsilon \sim \mathcal{N}(0,I)\).</p> </li> <li> <p>Forward diffuse \(x_t = \sqrt{\bar{\alpha_t}}x + \sqrt{1 - \bar{\alpha_t}} \epsilon\)</p> </li> <li> <p>For each conditioning input \(c_k \in \{c_i \}_{i=1}^n\), accumulate the loss as \(\text{Errors}[c_k] := \text{Error}[c_k] + \| \epsilon - \epsilon_\theta (x_t, c_k) \|^2\).</p> </li> </ol> <p>Finally, after all this is over, you can simply compute class assignments by:</p> \[c = \text{argmin}_{c_i}(\text{Error}[c_i])\] <p>Or you can just throw it all into a softmax to get a categorical distribution:</p> \[p_\theta(c | x) = \text{Softmax}(\text{Error})\] <h2 id="toy-experiment">Toy experiment</h2> <p>Let’s start with some data of two interlaced spirals (classes are red and blue):</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusion_classifier/gt_data-480.webp 480w,/assets/img/diffusion_classifier/gt_data-800.webp 800w,/assets/img/diffusion_classifier/gt_data-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/diffusion_classifier/gt_data.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Next, let’s train a diffusion model on this conditional data. Below is an example generation after it’s trained, where the red points are random noise conditioned on the red class and blue points are random noise conditioned on the blue class.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/img/diffusion_classifier/dt.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <p>Here’s the decision boundary of our classifier:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diffusion_classifier/decision_boundary-480.webp 480w,/assets/img/diffusion_classifier/decision_boundary-800.webp 800w,/assets/img/diffusion_classifier/decision_boundary-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/diffusion_classifier/decision_boundary.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It works pretty decently!</p>]]></content><author><name></name></author><category term="deep_learning_tricks"/><summary type="html"><![CDATA[A simple implementation on 2D points]]></summary></entry><entry><title type="html">Road to modern SSL Part 1, Ensembles, crops and augmentations</title><link href="adityamehrotra.ca/blog/EMA/" rel="alternate" type="text/html" title="Road to modern SSL Part 1, Ensembles, crops and augmentations"/><published>2024-08-11T15:59:00+00:00</published><updated>2024-08-11T15:59:00+00:00</updated><id>adityamehrotra.ca/blog/EMA</id><content type="html" xml:base="adityamehrotra.ca/blog/EMA/"><![CDATA[<p>I want to study each of a modern SSL pipeline’s components in <strong>isolation</strong>, until I’m able to understand the origins/reasoning for throwing all the components <strong>together</strong>.</p> <p>In this post, we’ll be studying the rationale and development of using an exponential moving average (EMA) of a neural network during SSL training.</p> <p>EMA is often used as a “teacher network”, for example see this figure from the DINO paper below:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ssl/dino_unannotated-480.webp 480w,/assets/img/ssl/dino_unannotated-800.webp 800w,/assets/img/ssl/dino_unannotated-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ssl/dino_unannotated.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="origins-in-deep-learning">Origins in deep learning</h2> <p>The first time I saw EMA was when I was reading the ADAM optimizer paper, where they kept an EMA of the gradients.</p> <p>EMA is defined as the following for some parameter \(\beta\in (0, 1)\):</p> \[v_t = \beta v_{t-1} + (1-\beta) o_t\] <p>Where \(v_t\) is our state at time \(t\) and \(o_t\) is our observation at timestep \(t\).</p> <p>Normally, we init with \(v_0 = 0\), this means our first “state” is going to be \(v_1 = (1-\beta)o_1\). With most choices of beta, this will be a downscaled version of the first observation.</p> <p>Therefore, we apply some bias correction by dividing our update rule by \((1-\beta^t)\):</p> \[v_t = \frac{\beta v_{t-1} + (1-\beta) o_t}{1-\beta^t}\] <p>The denominator quickly goes to ~1 and has no effect for future values of \(t\). But atleast for the first iteration, we get that:</p> \[v_1 = \frac{(1-\beta) o_t}{1-\beta} = o_t\] <p>Hence, we’ve fixed our issue of shifting the first observation down.</p> <p>I first saw this idea of bias correction in ADAM, section 3.</p> <h2 id="ladder-networks">Ladder networks</h2> <p>One of the papers which inspired using pairs of non-augmented/augmented images for SSL was Ladder networks.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ssl/ladder-480.webp 480w,/assets/img/ssl/ladder-800.webp 800w,/assets/img/ssl/ladder-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ssl/ladder.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>All you really need to understand is that, given an image \(x\), you feed it through an encoder \(f(\cdot)\) with random noise added to the activations \(\tilde{z}^{(i)}\) and get a label \(\tilde{y}\). Repeat the same pipeline with no noise added to get activations \(z^{(i)}\).</p> <p>Finally, want to train our decoder \(g(\cdot)\) to map \(\tilde{z}^{(i)}\) to \(\hat{z}^{(i)} = g(\tilde{z}^{(i)})\) that minimizes the MSE to the activations from the clean activations \(z^{(i)}\):</p> \[L= \left|\left|g(\tilde{z}^{(i)}) - z^{(i)}\right|\right|^2_2 =\left|\left|\hat{z}^{(i)} - z^{(i)}\right|\right|^2_2\] <p>Training labels here are completely optional, which is a bit of shortcoming. As the Mean Teacher paper states:</p> <blockquote> <p>Since the model itself generates targets, they may very well be incorrect. If too much weight is given to the generated targets, the cost of inconsistency outweighs that of misclassification, preventing the learning of new information.</p> </blockquote> <h2 id="self-supervision-with-temporal-ensembling">Self-supervision with temporal ensembling</h2> <h3 id="pi-networks">\(\Pi\) networks</h3> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ssl/pi-480.webp 480w,/assets/img/ssl/pi-800.webp 800w,/assets/img/ssl/pi-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ssl/pi.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The main idea is to add <em>different</em> noise to the input \(x\) with targets \(y\), to get \(x_1\) and \(x_2\), and put this into a network with dropout. Then you simply optimize the cross entropy on one of the samples, and minimize the squared difference between the logits \(z_1 = f(x_1)\) and \(z_2 = f(x_2)\) of both samples:</p> \[L = \text{cross_entropy}(z_1, y) + w(t)||z_1 - z_2||^2_2\] <p>You may also notice a weighting term \(w(t)\) on the MSE. This term starts off low, to allow the cross entropy term to help the model learn a supervised representation. It linearly increases in order to give more weightage to the SSL terms at later stages of the model training.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ssl/pi_alg-480.webp 480w,/assets/img/ssl/pi_alg-800.webp 800w,/assets/img/ssl/pi_alg-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ssl/pi_alg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="pi-network--temporal-ensembling">\(\Pi\) network + temporal ensembling</h3> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ssl/temp-480.webp 480w,/assets/img/ssl/temp-800.webp 800w,/assets/img/ssl/temp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ssl/temp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For temporal ensembling, we take our inputs \(x\) (with some data augmentation) and produce logits \(z = f(x)\). We also maintain an EMA of our past predictions on this datapoint \(x\). So say \(x\) as been seen 4 times in the previous 4 epochs with different augmentations (assuming each epoch cycles through the data with no replacement), we’ll have four different sets of logits \([z_1, z_2, z_3, z_4]\). We also have observed the logits \(z_5\), once we saw \(x\) on our current epoch 5.</p> <p>We then compute an EMA \(\tilde{z}_4\)of these past 4 logits, and we have the loss term</p> \[L = \text{cross_entropy}(z_5, y) + w(t)||z_5 - \tilde{z}_4||^2_2\] <p>Then, we update our EMA for this datapoint:</p> \[\tilde{z}_5 = \beta \tilde{z}_4 + (1-\beta)z_5\] <p>Of course, we don’t actually keep a storage of logits \([z_1, z_2, \dots, z_N]\) for each time we see datapoint \(x\), we can just store the EMA and update it in-place.</p> <p>The main idea here is that, the EMA offers a more robust embedding since it incorporates previous context/models. By ensembling these embeddings via EMA, and supervising on that, we can observe much better performance.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ssl/temp_results-480.webp 480w,/assets/img/ssl/temp_results-800.webp 800w,/assets/img/ssl/temp_results-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ssl/temp_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here’s the full algorithm if you’re interested</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ssl/temp_alg-480.webp 480w,/assets/img/ssl/temp_alg-800.webp 800w,/assets/img/ssl/temp_alg-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ssl/temp_alg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here’s the big issues with this:</p> <ul> <li>The time it takes for the EMA to actually be rich enough to provide a good representation takes a lot of time. Say it takes 10 logits in the EMA for a datapoint to actually be useful, that means 10 epochs, so this is roughly \(O(10\cdot D)\). This can be a LOT of time if \(D\) is large.</li> <li>For modern datasets with many classes, this means you need to keep \(O(C^2)\) floats in memory. Where \(C\) is the number of classes and dimension of the logits.</li> </ul> <h2 id="mean-teachers-are-better-role-models">Mean teachers are better role models</h2> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ssl/mean-480.webp 480w,/assets/img/ssl/mean-800.webp 800w,/assets/img/ssl/mean-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ssl/mean.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The idea here is simple. Given an input \((x,y)\), student parameters \(\theta_{student}\) and teacher parameters \(\theta_{teacher}\). Compute their respective outputs \(f(x; \eta_{student}, \theta_{student})\), \(f(x; \eta_{teacher}, \theta_{teacher})\), where the \(\eta 's\) are some gaussian noise that we add to the layers of both models.</p> <p>Then, compute a classification loss on the student logits, and a consistency loss on the teacher and student logits.</p> \[L = \text{cross_entropy}(f(x; \eta_{student}, y)) + ||f(x; \eta_{student}, \theta_{student}) - f(x; \eta_{teacher}, \theta_{teacher})||_2^2\] <p>Finally, we update the student model with GD and update the EMA of our teacher model:</p> \[\theta_{student} = \theta_{student} - \alpha \nabla_L(\theta{student})\] \[\theta_{teacher} = \beta \theta_{teacher} + (1-\beta) \theta_{student}\] <p>The main idea here is that we keep a EMA of the student models, and use that as the teacher.</p> <p>Turns out this ​empirically did much better than state of the art, especially in the low-label count regime.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ssl/mean_graph-480.webp 480w,/assets/img/ssl/mean_graph-800.webp 800w,/assets/img/ssl/mean_graph-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ssl/mean_graph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This fixes the main issue that temporal ensembling had with slow updates + data storage, as now the teacher is being updated each weight update, and we don’t need to store a lot of EMAs. The only requirement is that the student fits in VRAM twice.</p> <h2 id="wrap-up">Wrap up</h2> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ssl/dino_annot-480.webp 480w,/assets/img/ssl/dino_annot-800.webp 800w,/assets/img/ssl/dino_annot-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ssl/dino_annot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This roughly traces the paper trail for what inspired the EMA + two image augmentation for modern SSL. The goal is to offer continous per-batch supervision using a teacher model, which turns out to work well as an EMA. Additionally different augmentation is to keep the model features robust to image perturbations.</p> <p>The questions I have after this is:</p> <ul> <li>In what situations does this break? I guess that this all is very sensitive to \(\beta\), the choice of augmentations, different schedules for hyperparameters etc… I’m wondering what causes this framework to become unstable.</li> <li>Mean teacher has explicit supervision on the targets, but modern SSL does not. How did we remove reliance on labels entirely?</li> </ul>]]></content><author><name></name></author><category term="deep_learning_tricks"/><summary type="html"><![CDATA[Learning modern SSL, one piece at a time]]></summary></entry><entry><title type="html">An introduction to Slot Attention</title><link href="adityamehrotra.ca/blog/Slot-Attention/" rel="alternate" type="text/html" title="An introduction to Slot Attention"/><published>2024-01-22T00:00:00+00:00</published><updated>2024-01-22T00:00:00+00:00</updated><id>adityamehrotra.ca/blog/Slot-Attention</id><content type="html" xml:base="adityamehrotra.ca/blog/Slot-Attention/"><![CDATA[<p>In this blog post, I’ll cover the basic slot attention mechanism <a class="citation" href="#locatello2020objectcentric">(Locatello et al., 2020)</a> and go over some intuition as to why it works.</p> <p>The problem we tackle is learning representations for particular regions of images, like a representation for a box, sphere or background of the scene.</p> <h2 id="intuition-k-means-clustering">Intuition, k-means clustering</h2> <h3 id="hard-k-means-clustering">Hard K-means clustering:</h3> <p>K-means clustering in the most simple form has the following steps:</p> <p><strong>Setup</strong>: Randomly initialize clusters by assigning each vector \(\mathbf{x}_n\) to one of \(N\) clusters</p> <p><strong>Repeat</strong>:</p> <ol> <li>Compute cluster means \(\boldsymbol{\mu}_n\) by averaging all points assigned to cluster \(n\)</li> <li>Reassign each each point to the cluster corresponding to it closest mean \(\boldsymbol{\mu}_k\).</li> <li>If some no new assignments were made, we’ve converged and can stop.</li> </ol> <p>While the example I gave above was for 2D Points, this algorithm has been used for images as well, for segmenting regions of the image by the pixel values in some colour space.</p> <p>When this algorithm converges, we expect to have \(N\) centroids, where each one should have a high affinity to the points closest to it.</p> <h3 id="soft-k-means-clustering">Soft K-Means clustering</h3> <p>The above algorithm doesn’t take into account that a certain point may share characteristics of more than one cluster. Hard K-Means is not expressive enough to capture this sort of relation, so we use Soft K-Means instead.</p> <p><strong>Setup</strong>: Randomly initialize clusters by assigning each vector \(\mathbf{x}_n\) to one of \(N\) clusters. We also define a distance measure \(\phi_n(i)\), that measures the “closeness” of \(\mathbf{x}_n\) to cluster \(\boldsymbol{\mu}_i\).</p> <p><strong>Repeat</strong>: Iterate the following</p> <ol> <li> <p>Updating \(\phi\):</p> \[\phi_n(i)=\frac{\exp \left\{-\frac{1}{\beta}\left\|\mathbf{x}_n-\boldsymbol{\mu}_i\right\|^2\right\}}{\sum_j \exp \left\{-\frac{1}{\beta}\left\|\mathbf{x}_n-\boldsymbol{\mu}_j\right\|^2\right\}}, \text { for } i=1, \ldots, N\] </li> <li> <p>Update \(\boldsymbol{\mu}_i\) : For each \(i\), update \(\boldsymbol{\mu}_k\) with the weighted average</p> </li> </ol> \[\boldsymbol{\mu}_i=\frac{\sum_n \mathbf{x}_n \phi_n(i)}{\sum_n \phi_n(i)}\] <p>Now we have a more expressive model, but notice a few things:</p> <ul> <li>The model is dependent on the data that it has fit to. Eg: Say you run this on image A and get some clusters. In order to recieve new clusters, you need to rerun the algorithm.</li> <li>Highly dependent on initialization. For some cluster initializations, you can get a very poor clustering performance</li> <li>The cluster mean might not be the best representation of the vectors inside the cluster itself</li> </ul> <p>This motivates the question, can we use parameters and highly expressive neural networks to perform much better than K-Means?</p> <h2 id="slot-attention">Slot Attention</h2> <h3 id="how-to-compute-slots">How to compute slots:</h3> <p>The slot attention operation works as follows:</p> <p><strong>Setup:</strong> Initialize embedding weights for key, query and value projection \(q( \cdot ), k(\cdot), v(\cdot)\) <a class="citation" href="#vaswani2023attention">(Vaswani et al., 2023)</a>. Also initialize \(N_\text{slots}\) slots of embedding dimension \(D\), basically a matrix of shape \(N_{\text{slots}} \times D\). These can be sampled from an isotropic normal with learned mean \(\boldsymbol{\mu} \in \mathbb{R}^{D}\)</p> <p><strong>Repeat T times</strong>:</p> <ol> <li>Given image of shape \(B\times 3 \times H \times W\), use a CNN encoder to encode this into a feature map of dimension \(B\times D \times H \times W\). Then flatten this into a sequence of tokens of shape \(N_{\text{data}} \times D\), call this \(\text{inputs}\).</li> <li> <p>Compute softmax weights over the data embeddings:</p> \[\text{Softmax}(\frac{1}{\sqrt{D}} k(\text{inputs}) q(\text{slots})^T, \text{axis='slots'})\] <p>Unpacking this, we have \(\text{inputs} \in \mathbb{R}^{N_{data} \times D}\) and \(\text{slots} \in \mathbb{R}^{N_{slots} \times D}\) (excluding batch dimension). Therefore, our softmax weights are of shape \((N_{\text{data}}, N_{\text{slots}})\) and the softmax is done across the \(N_{\text{slots}}\) axis. This is different from regular attention, which is done over the \(N_{\text{data}}\) axis, as this promotes “competition” across the slots. I’ll explain the intuition for that later.</p> </li> <li> <p>Compute slot updates as using a weighted average, the shape of which is \((N_{\text{slots}},D)\). Note that is the same shape as our slots.</p> \[\text{Softmax}(\frac{1}{\sqrt{D}} k(\text{inputs}) q(\text{slots})^T, \text{axis='slots'})^T v(\text{inputs})\] </li> <li>Update the previous slots using a small GRU network and the slot updates as the input.</li> </ol> <h3 id="intuition">Intuition</h3> <p>As you can see above, each slot update is a linear combination of \(\text{inputs}\). The coefficients of input embedding vector is determined by the softmax matrix, very similar to the regular attention mechanism we all know and love.</p> <p>To see why the slots enforce competition, we need to take a look at the softmax matrix in more detail. Denote \(I_i\) as the \(i^{th}\) input vector, and \(S_j\) as the \(j^{th}\) slot vector.</p> \[k(\text{inputs}) q(\text{slots})^T = \begin{pmatrix} I_1\cdot S_1 &amp; \cdots &amp; I_1 \cdot S_{N_{\text{slots}}} \\ \vdots &amp; \ddots &amp; \vdots \\ I_{N_{\text{data}}} \cdot S_1 &amp; \cdots &amp; I_{N_{\text{data}}}\cdot S_{N_{\text{slots}}} \end{pmatrix}\] <p>For which direction to take the softmax in, we have two options, either row wise (on the data axis) or column wise (on the slot axis):</p> <ul> <li> <p>If we normalize across the data axis, each row will sum up to one. So when we right multiply by \(v(\text{inputs})\), each slot update will be a convex linear combination of the input vectors. This is exactly what is used in the regular attention mechanism. However, each slot is unrestricted in what parts of the input sequence it can attend to. For example, the first embedding could have a softmax weight of \(1.0\).</p> </li> <li> <p>If we normalize across the slot dimension, each column will sum up to one. Now when we right multiply by \(v(\text{inputs})\), each slot update won’t be a convex linear combination anymore. However, now we are constraining the attention weights for each embedding across all slots. For example, if slot \(S\) has a high attention weight \(\approx 1\) for embedding \(I\), then it must be the case that the other slots have attention weights \(\approx 0\) for \(I\). This promotes “competition” for input vectors among the slots, as only a few slots will be able to have a high weight for any given input vector due to the properties of softmax.</p> </li> </ul> <p>Ultimately, if a slot has high coefficients for a set of input embedding vectors, it should be representative of those input embedding vectors.</p> <p>Therefore, I view this as a more expressive version of the K-Means operation we covered earlier, as the goal of both is to compute embeddings for distinct regions of the input images, and both do so via linear combinations of the input data. I believe expressive comes from the high degree of nonlinearity within the \(q,k,v\) projections and \(\text{Softmax}\).</p> <h3 id="what-does-this-actually-do">What does this actually do?</h3> <p>So now we run slot attention on a given image, and receive \(N_{\text{slots}}\) slots. What do we do now?</p> <p>Well, we need some sort of signal to update these weights, and quantify how good of a representation we learned. A simple answer (and what is used in the original paper) is to simply reconstruct the original image.</p> <p>So assume each slot has learned to have high affinity (high inner product) with a particular region of the image. Eg: One slot binded to a sphere, another to a cube. Then, if we reconstruct each slot into an image, we should get a set of images for each object the slot binds to.</p> <p>This is exactly what is done in the paper. Here is the sequence of steps to decode:</p> <ol> <li>For each slot of shape \((D, )\), use add two new spatial dimensions and repeat, getting a shape of \((D, \hat{H}, \hat{W})\)</li> <li>Run this through a series of transpose convolutions, to get images of shape \((4, H, W)\).</li> <li>The first 3 channels are RGB respectively, the last is an alpha channel. So for each slot \(i \in \{1, \dots, K\}\), we split each feature map into \(C_i \in \mathbb{R}^{3\times H \times W}\) and \(\alpha_i \in \mathbb{R}^{3 \times H \times W}\) (we simply took our alpha channel and repeated it 3 times, so the shape lines up with \(C_i\)).</li> <li>Finally we compose these by to get predicted image \(\hat{Y}\).</li> </ol> \[\hat{Y} = \sum_{i=1}^K \alpha_i \odot C_i\] <p>Our loss function is the simple MSE loss between original image \(Y\) and our predicted image \(\hat{Y}\):</p> \[L(\hat{Y}, Y) = \left | \left | \hat{Y} - Y \right | \right |^2_2\] <h3 id="visuals">Visuals:</h3> <p>Here’s a visual of the pipeline I presented above for this image with 7 objects and 7 slots. While the decoded image isn’t perfect, notice how each slot representation, when decoded, has roughly learned to represent a specific object in the scene.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://github.com/AditMeh/AditMeh.github.io/blob/master/assets/img/slot-attention/slot_attn_diagram?raw=true-480.webp 480w,https://github.com/AditMeh/AditMeh.github.io/blob/master/assets/img/slot-attention/slot_attn_diagram?raw=true-800.webp 800w,https://github.com/AditMeh/AditMeh.github.io/blob/master/assets/img/slot-attention/slot_attn_diagram?raw=true-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="https://github.com/AditMeh/AditMeh.github.io/blob/master/assets/img/slot-attention/slot_attn_diagram.png?raw=true" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="shortcomings-and-future-directions">Shortcomings and future directions:</h3> <p>To be added later…</p>]]></content><author><name></name></author><category term="computer_vision"/><summary type="html"><![CDATA[Going over the basics of Slot Attention, and covering some recent literature in the area]]></summary></entry><entry><title type="html">Inverse Transform Sampling in NeRF</title><link href="adityamehrotra.ca/blog/NeRF-Inverse-Transform-Sampling/" rel="alternate" type="text/html" title="Inverse Transform Sampling in NeRF"/><published>2023-08-06T15:59:00+00:00</published><updated>2023-08-06T15:59:00+00:00</updated><id>adityamehrotra.ca/blog/NeRF-Inverse-Transform-Sampling</id><content type="html" xml:base="adityamehrotra.ca/blog/NeRF-Inverse-Transform-Sampling/"><![CDATA[<h2 id="problem-setup">Problem Setup</h2> <p>I was working on my implementation of NeRF and I read section 5.2 (Hierarchical Sampling). As discussed in the paper we have t-values \((t_1, \dots, t_{N_c})\), and we compute weights \(w_1, \dots, w_{N_c}\) as:</p> \[w_i=T_i\left(1-\exp \left(-\sigma_i \delta_i\right)\right)\] <p>They then normalize these weights to produce a piecewise constant PDF:</p> \[\hat{w}_i = \frac{w_i}{\sum_{j=1}^{N_c} w_j}\] <p>The authors propose using inverse transform sampling to sample t-values along the this ray around points that have a high \(\hat{w}_i\).</p> <p>When I read this my understanding of inverse transform sampling was limited to what’s described <a href="https://stephens999.github.io/fiveMinuteStats/inverse_transform_sampling.html#discrete_distributions">here</a>.</p> <p>However, none of these sampling techniques apply here because we don’t have the analytic form of an inverse CDF \(F^{-1}(x)\).</p> <p>Therefore, we need to construct this CDF and a method of evaluating it.</p> <h2 id="a-first-pass">A first pass</h2> <p>Let’s start simple, lets say we have the following ordered set of (t-value, \(\hat{w}\)) pairs \((t_1, \hat{w}_1), \dots, (t_{N_c}, \hat{w}_{N_c})\), such that \(\sum_{j=1}^{N_c} \hat{w}_j = 1\) and \(t_i &lt; t_{i+1}\).</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hierarchical/figure1-480.webp 480w,/assets/img/hierarchical/figure1-800.webp 800w,/assets/img/hierarchical/figure1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/hierarchical/figure1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Okay, now let’s compute the cumulative sum. We will now have points \((t_1, \bar{w}_1), \dots, (t_{N_c}, \bar{w}_{N_c})\) where \(\bar{w}_i = \sum_{j=1}^{i} \hat{w}_j\) (note the last point should have a height of 1).</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hierarchical/figure2-480.webp 480w,/assets/img/hierarchical/figure2-800.webp 800w,/assets/img/hierarchical/figure2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/hierarchical/figure2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Now, I want to somehow use this to perform inverse transform sampling, that is, mapping \(u \sim U[0,1]\) to a t-value \(t_u\).</p> <p>My first guess is to find the neighboring points on the cumulative sum function \((t_{left}, \bar{w}_{left}), (t_{right}, \bar{w}_{right})\) such that \(\bar{w}_{left} \leq t \leq \bar{w}_{right}\). With this, we can compute \(t_u\) as:</p> \[t_u = (t_{right} - t_{left}) \cdot r + t_{left}, \hspace{0.5cm} r= \frac{u - \bar{w}_{left}}{\bar{w}_{right} - \bar{w}_{left}}\] <p>Here are some visualizations of this procedure:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hierarchical/figure3-480.webp 480w,/assets/img/hierarchical/figure3-800.webp 800w,/assets/img/hierarchical/figure3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/hierarchical/figure3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>However, notice what happens if \(u &lt; \bar{w}_1\), we cannot find a \(\bar{w}_{left}\) such that \(\bar{w}_{left} \leq u\). Therefore this method doesn’t work when \(u &lt; \bar{w}_1\).</p> <h2 id="an-online-solution">An online solution</h2> <p>I look around on the internet for how people implement this, and I found Krishna Murthy’s <a href="https://github.com/krrish94">NeRF Implementation</a>. The rest of the post will be going over how this is implemented.</p> <p>Krishna Murthy instead does the following:</p> <ol> <li>Take your t-values \((t_1, \dots, t_{N_c})\) and compute a new list of \(N_c-1\) points</li> </ol> \[\left (\frac{t_2 + t_1}{2}, \dots , \frac{t_{N_c} + t_{N_{c} - 1}}{2} \right )\] <ol> <li> <p>Take weights \((w_2, \dots, w_{N_c - 1})\). Note that they are not normalized. then replace each \(w_i\) with \(\hat{w}_i = \frac{w_i}{\sum_{j=2}^{N_c - 1} w_i}\). This is a list of \(N_c - 2\) probabilities. Append a 0 to the beginning of the list to get \((0, \hat{w}_2, \dots, \hat{w}_{N_c -1})\). Finally like before, replace each \(\hat{w}_i\) with \(\bar{w}_i = \sum_{j=2}^{i} \hat{w}_j\) to get \((0, \bar{w}_2, \dots, \bar{w}_{N_c -1})\). We now have \(N_c - 1\) probabilities.</p> </li> <li> <p>Pair up these \(N_c -1\) points and probabilities to get:</p> </li> </ol> \[\left(\left(\frac{t_{2} + t_{1}}{2}, 0 \right), \dots, \left(\frac{t_{N_c} + t_{N_{c} - 1}}{2}, \bar{w}_{N_c - 1} \right)\right)\] <p>Notice that the first point has a y value of 0, and the last has a y value of \(\bar{w}_{N_c-1} =1\) by definition. We now are able to find a left and right neighbor for all \(u \in [0,1]\), solving our previous problem.</p> <p>Let’s visualize what this function looks like compared to our previous one.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hierarchical/figure4-480.webp 480w,/assets/img/hierarchical/figure4-800.webp 800w,/assets/img/hierarchical/figure4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/hierarchical/figure4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As you can see, it’s very similar to our previous attempt at a CDF and puts a lot of density around the regions with t-values that have a high probability. To illustrate this, I sampled 100 values \((u_1, ..., u_100)\) and computed \(t_{u_i}\) for each of them using the inverse transform sampling scheme I described earlier. On the left of the plot below, Each \(u_i\) is plotted in blue on the y-axis, and the corresponding \(t_{u_i}\)’s are also plotted in blue on the x axis.</p> <p>On the right of the plot below, I’ve plotted the original t-values and their normalized weights, exactly the same as the first figure in this blogpost. However, I’ve added the green dots on the x-axis which correspond to each of the \(t_{u_i}\)’s. As you can see, most of them lie in high density regions of the ray. This shows that with this method, we’re able to sample t-values from high density regions along the ray using inverse transform sampling.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hierarchical/figure5-480.webp 480w,/assets/img/hierarchical/figure5-800.webp 800w,/assets/img/hierarchical/figure5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/hierarchical/figure5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="computer_vision"/><summary type="html"><![CDATA[Going over an implementation of inverse transform sampling in NeRF]]></summary></entry><entry><title type="html">Classifier-free diffusion guidance</title><link href="adityamehrotra.ca/blog/diffusion_guidance/" rel="alternate" type="text/html" title="Classifier-free diffusion guidance"/><published>2023-03-05T15:59:00+00:00</published><updated>2023-03-05T15:59:00+00:00</updated><id>adityamehrotra.ca/blog/diffusion_guidance</id><content type="html" xml:base="adityamehrotra.ca/blog/diffusion_guidance/"><![CDATA[<p>In this post, I’ll describe a simple method on how to condition diffusion models called classifier-free guidance. I won’t go into the SDE side of things because I don’t understand it yet.</p> <h2 id="score-function">Score function</h2> <p>The score function for an arbitrary step along our markov chain \(\textbf{x}_t\) is defined as \(s_\theta(\textbf{x}_t, t) = \nabla_{\textbf{x}_t} \log q(\textbf{x}_t)\). Intuitively, this quantity tells us how to change the noisy \(\textbf{x}_t\) to make it more likely under the true data distribution.</p> <p>In diffusion models, we have a forward diffusion distribution \(q(\textbf{x}_t \mid \textbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha_t}} \textbf{x}_0, (1- \bar{\alpha_t}) \textbf{I})\). The gradient of the \(\log\) pdf of a gaussian \(\mathcal{N}(\textbf{x}; \mu, \sigma^2)\) can be computed as:</p> \[\begin{align*} &amp; \nabla_{\textbf{x}} p(\textbf{x}) = \nabla_\textbf{x} \left ( - \frac{1}{2 \sigma^2} (\textbf{x} - \mathbf{\mu})^2 \right) = -\frac{\textbf{x} - \mathbf{\mu}}{\sigma^2}\\ &amp;= -\frac{\mathbf{\epsilon}}{\sigma} \hspace{2 cm} (\textbf{x} = \mathbf{\mu} + \sigma \odot \mathbf{\epsilon}, \mathbf{\epsilon} \sim \mathcal{N}(\textbf{0, I})) \end{align*}\] <p>Therefore, we can express the score function of a sample \(\textbf{x}_t\) as:</p> \[\begin{align*} &amp;\mathbf{s}_\theta(\mathbf{x}_t, t) \approx \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t) = - \frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}} \end{align*}\] <p>I simply plugged the variance of \(q(\textbf{x}_t \mid \textbf{x}_0)\) and the diffusion model we train \(\mathbf{\epsilon}_{\theta} (\textbf{x}_t, t)\) is supposed to match the noise \(\mathbf{\epsilon}\), so I substitute that in as well.</p> <p>Therefore, I now have an equivalence of the score function in terms of our diffusion model’s U-Net. Learning the diffusion model as stated in DDPM is the same thing as learning the score function.</p> <h2 id="classifier-guidance">Classifier Guidance</h2> <p>Given our equivalence of the score function and noise prediction network, we can intuitively understand conditioning.</p> <p>If we have some auxillary input \(y\) that we want to condition on, we the need to model the score function \(\nabla_{\textbf{x}_t} \log q(\textbf{x}_t \mid \textbf{y})\). Hence, using bayes rule we can write this as:</p> \[\begin{align*} &amp; q(\textbf{x}_t \mid \textbf{y}) = \frac{q(\textbf{y} \mid \textbf{x}_t) q(\textbf{x}_t )}{q(\textbf{y})} \\ &amp; \implies \log q(\textbf{x}_t \mid \textbf{y}) = \log q(\textbf{y} \mid \textbf{x}_t) + \log q(\textbf{x}_t ) - \log q(\textbf{y}) \\ &amp; \implies \nabla_{\textbf{x}_t} \log q(\textbf{x}_t \mid \textbf{y}) = \nabla_{\textbf{x}_t} \log q(\textbf{y} \mid \textbf{x}_t) + \nabla_{\textbf{x}_t} \log q(\textbf{x}_t ) \end{align*}\] <p>It’s evident here that \(\nabla_{\textbf{x}_t} \log q(\textbf{y} \mid \textbf{x}_t)\) can be computed using a differentiable approximator, such as a softmax classifier (in the case of labels). We can add a hyperparameter \(s\) (called “guidance”), which controls how much influence this classifier has on our final prediction.</p> \[\nabla_{\textbf{x}_t} \log q(\textbf{x}_t \mid \textbf{y}) = \nabla_{\textbf{x}_t} \log q(\textbf{x}_t ) + s \cdot \nabla_{\textbf{x}_t} \log q(\textbf{y} \mid \textbf{x}_t)\] <p>The issue is, our \(\textbf{x}_t\) can be arbitrarily noisy and our classifier will not be able to be accurate at high levels of noise.</p> <h2 id="classifier-free-guidance">Classifier-Free Guidance</h2> <p>Hence, we seek to eliminate our dependence on a classifier, so we use bayes rule once again in the other direction:</p> \[\begin{align*} &amp; q(\textbf{y} \mid \textbf{x}_t) = \frac{q(\textbf{x}_t \mid \textbf{y}) q(\textbf{y})}{q(\textbf{x}_t)} \\ &amp; \implies \log q(\textbf{y} \mid \textbf{x}_t) = \log q(\textbf{x}_t \mid \textbf{y}) + \log q(\textbf{y}) - \log q(\textbf{x}_t) \\ &amp; \implies \nabla_{\textbf{x}_t} \log q(\textbf{y} \mid \textbf{x}_t) = \nabla_{\textbf{x}_t} \log q(\textbf{x}_t \mid \textbf{y}) - \nabla_{\textbf{x}_t} \log q(\textbf{x}_t) \\ \end{align*}\] <p>Plugging this back into our equation from Classifier Guidance:</p> \[\begin{align*} \nabla_{\textbf{x}_t} \log q(\textbf{x}_t \mid \textbf{y}) &amp;= \nabla_{\textbf{x}_t} \log q(\textbf{x}_t ) + s \cdot (\nabla_{\textbf{x}_t} \log q(\textbf{x}_t \mid \textbf{y}) - \nabla_{\textbf{x}_t} \log q(\textbf{x}_t)) \\ &amp;= (1-s) \cdot \nabla_{\textbf{x}_t} \log q(\textbf{x}_t ) + s \cdot \nabla_{\textbf{x}_t} \log q(\textbf{x}_t \mid \textbf{y}) \end{align*}\] <p>Ultimately, using our discussion of score functions earlier, we can equate this to learning a diffusion model as:</p> \[\begin{align*} \hat{\epsilon}(\textbf{x}_t, \textbf{y}, t) &amp;= -\frac{1}{\sqrt{1 - \bar{\alpha}_t}} \left ( (1-s) \cdot \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) + s \cdot \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, \mathbf{y}, t) \right) \end{align*}\] <p>We <em>could</em> just model \(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, \mathbf{y}, t)\) directly, however this formulation allows us to have more fine grained control over how much our learned conditional distribution affects the final generated sample.</p> ]]></content><author><name></name></author><category term="generative_models"/><summary type="html"><![CDATA[A short derivation of classifier free diffusion guidance]]></summary></entry><entry><title type="html">Torch buffer vs Parameter</title><link href="adityamehrotra.ca/blog/buffervsparameter/" rel="alternate" type="text/html" title="Torch buffer vs Parameter"/><published>2023-01-05T15:59:00+00:00</published><updated>2023-01-05T15:59:00+00:00</updated><id>adityamehrotra.ca/blog/buffervsparameter</id><content type="html" xml:base="adityamehrotra.ca/blog/buffervsparameter/"><![CDATA[<h1 id="torch-parameter-vs-buffer">Torch parameter vs buffer</h1> <h3 id="torch-parameters">Torch Parameters</h3> <p>You can create a parameter in your torch model by using the following in your init method.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)))</span></code></pre></figure> <p>When I save the model’s <code class="language-plaintext highlighter-rouge">state_dict</code>, I’ll find this within the <code class="language-plaintext highlighter-rouge">model.parameters()</code>. However, what if we had something that didn’t need gradient and hence did not need to be a parameter? An example would the mean and variance used in batch normalization. That’s where the next idea comes into play:</p> <h3 id="torch-buffers">Torch Buffers</h3> <p>You can create a buffer in your torch model by using the following in your init method of your <code class="language-plaintext highlighter-rouge">nn.Module</code></p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Usage: self.register_buffer(k, v)
</span><span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">"</span><span class="s">some_tensor</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span></code></pre></figure> <p>In this example, <code class="language-plaintext highlighter-rouge">k</code> is a string <code class="language-plaintext highlighter-rouge">"some_tensor"</code> and <code class="language-plaintext highlighter-rouge">v</code> is a tensor of ones. I can now access this tensor via <code class="language-plaintext highlighter-rouge">self.some_tensor</code>, kind of like a python dictionary. The ones tensor never recieves and gradient, and will be stored in your state dict.</p> <h3 id="conclusion">Conclusion</h3> <p>So to wrap up, you should use parameters for things that require gradient and buffers for things that don’t. Of course, instead of buffers you can use <code class="language-plaintext highlighter-rouge">nn.Parameter</code> and set <code class="language-plaintext highlighter-rouge">requires_grad = False</code>, but your optimizer will need to check the <code class="language-plaintext highlighter-rouge">requires_grad</code> attribute of these tensors during the weight update, which is an unnecesary step.</p>]]></content><author><name></name></author><category term="programming"/><summary type="html"><![CDATA[A short writeup about the difference between torch buffers and parameters]]></summary></entry><entry><title type="html">Deriving the ELBO for VAEs</title><link href="adityamehrotra.ca/blog/introduction-to-VAEs/" rel="alternate" type="text/html" title="Deriving the ELBO for VAEs"/><published>2022-11-15T15:59:00+00:00</published><updated>2022-11-15T15:59:00+00:00</updated><id>adityamehrotra.ca/blog/introduction-to-VAEs</id><content type="html" xml:base="adityamehrotra.ca/blog/introduction-to-VAEs/"><![CDATA[<p>Assume we have some log likelihood \(\log(p_\theta(x))\) we want to maximize, where the parameters of our probablistic model can be denoted as \(\theta\). Now, we express it in terms of joint probability of \(p_\theta(x, z)\) as:</p> \[\log \left ( \int_z p_\theta (x,z) dz \right )\] <p>We call \(z\) a “latent variable”. In the case that we have multiple latent variables in a vector \(\mathbf{z} \in \mathbb{R}^n\), we can write</p> \[\log \left ( \int_\mathbf{z} p_\theta (x,z_1, z_2, \dots, z_n) d\mathbf{z} \right )\] <p>This is intractable, as this requires way too many \(\mathbf{z}\) values to get a good enough approximation.</p> <p>Hence, if we lower bound this with something that IS tractable, then we’re able to optimize this.</p> <p>Since we have a joint probability \(p_\theta(x, \mathbf{z})\), we want to somehow decompose this into a product of two probabilities using the product rule. Below, you can see the graphical model, which tells us how we should be decomposing the joint distribution.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/elbo/VAE_graphical_model-480.webp 480w,/assets/img/elbo/VAE_graphical_model-800.webp 800w,/assets/img/elbo/VAE_graphical_model-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/elbo/VAE_graphical_model.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Graphical model of the VAE </div> <p>Therefore by decomposing the joint according to the graphical model:</p> \[= \log \left( \int_{\mathbf{z}} p_\theta (x \mid \mathbf{z}) p(\mathbf{z}) d\mathbf{z} \right )\] <p>We refer to \(p(\mathbf{z})\) as the “prior”, which is a distribution over our latent variables. Notice I don’t subscript this with \(\theta\), because it doesn’t have any paramters and is just a fixed distribution.</p> <p>Now, I will make a distinction. There are two distributions we are trying to learn: \(p(x \mid \mathbf{z})\) and \(p( \mathbf{z} \mid x)\). Overriding my previous notation, I denote \(\theta\) as the parameters for \(p_\theta(x \mid \mathbf{z})\) and \(\phi\) as the parameters for \(p_\phi(\mathbf{z} \mid x)\).</p> <p>Hence, going back to the log-likelihood, I multiply the insides by \(\frac{p_\phi (\mathbf{z} \mid x)}{p_\phi (\mathbf{z} \mid x)} = 1\).</p> \[= \log \left( \int_{\mathbf{z}} p_\theta (x \mid \mathbf{z}) p(\mathbf{z}) \cdot \frac{p_\phi (\mathbf{z} \mid x)}{p_\phi (\mathbf{z} \mid x)} d\mathbf{z} \right )\] <p>Then, I convert this into an expectation:</p> \[\log \left( \mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)}\left [ p_\theta (x \mid \mathbf{z}) \cdot \frac{p(\mathbf{z})}{p_\phi (\mathbf{z} \mid x)} \right ]\right )\] <p>At this point, we can apply jensen’s inequality, which tells us that \(\log \left( \mathbb{E}\left [X \right] \right ) \geq \mathbb{E}\left [\log(X) \right]\). Applying this gets us:</p> \[\log \left( \mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)}\left [ p_\theta (x \mid \mathbf{z}) \cdot \frac{p(\mathbf{z})}{p_\phi (\mathbf{z} \mid x)} \right ]\right ) \geq \mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)}\left [ \log \left( p_\theta (x \mid \mathbf{z}) \cdot \frac{p(\mathbf{z})}{p_\phi (\mathbf{z} \mid x)} \right ) \right ]\] <p>I will then simplify the right side:</p> \[\begin{align*} &amp;= \mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)} \left [ \log \left( p_\theta (x \mid \mathbf{z}) \right) + \log \left( p(\mathbf{z}) \right) - \log \left (p_\phi (\mathbf{z} \mid x) \right ) \right ] \\ &amp;=\mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)} \left [ \log \left( p_\theta (x \mid \mathbf{z}) \right) \right ] + \mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)} \left [ \log \left( p(\mathbf{z}) \right) \right ] - \mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)} \left [ \log \left (p_\phi (\mathbf{z} \mid x) \right ) \right ] \\ &amp;=\mathbb{E}_{\mathbf{z} \sim p_\phi (\mathbf{z} \mid x)} \left [ \log \left( p_\theta (x \mid \mathbf{z}) \right) \right ] - \text{KL} \left[ p(\mathbf{z})\mid \mid p_\phi (\mathbf{z} \mid x) \right ] \end{align*}\] <p>Where \(\text{KL}\) is the \(\text{KL}\) divergence between the prior \(p(\mathbf{z})\) and the posterior \(p_\phi(\mathbf{z} \mid x)\)</p>]]></content><author><name></name></author><category term="generative_models"/><summary type="html"><![CDATA[A simple derivation of the ELBO]]></summary></entry><entry><title type="html">Knowledge Distillation</title><link href="adityamehrotra.ca/blog/knowledge_dist/" rel="alternate" type="text/html" title="Knowledge Distillation"/><published>2022-04-15T15:59:00+00:00</published><updated>2022-04-15T15:59:00+00:00</updated><id>adityamehrotra.ca/blog/knowledge_dist</id><content type="html" xml:base="adityamehrotra.ca/blog/knowledge_dist/"><![CDATA[<p>This post will go over the math and mechanics of how <a href="https://arxiv.org/abs/1503.02531">Knowledge Distillation</a> works and also include some code on how to implement it.</p> <h2> Preliminaries:</h2> <p>Say you have a classification task, where you have some feedforward net to classify the digits of MNIST. Denote the number of samples in the minibatch as \(m\), so we index a sample with \(i \in [1, m]\) and also denote the number of output classes as \(n \in \mathbb{N}\). The feedforward net produces unnormalized class scores (final output from the model), which we will denote as a vector \(z_i \in \mathbb{R}^n\). Finally, we retrieve a probability distribution over the output classes using the softmax function applied to the \(z_i\), we will denote this as \(a_i \in \mathbb{R}^n\). Finally, I will use an arbitrary \(k \in [0, n-1]\) to index my vectors. So \(z_{i,k}\) represents the \(k^{th}\) element of \(z_i\), and likewise for \(a_i\). Here is the formula:</p> \[\LARGE{a_{i,k} := \frac{e^{z_{i,k}}}{\sum_{j=0}^{n-1} e^{z_{i,j}}}}\] <p>Here is an example of it being used (rounded to four digits): </p> \[\begin{align*} \small{\begin{bmatrix} 2 \\ 10 \\ 3 \\ 0 \\ 5 \\ 4 \\ 7 \\ 9 \\ 1 \\ 2 \end{bmatrix}} \Longrightarrow softmax\left(\begin{bmatrix} 2 \\ 10 \\ 3 \\ 0 \\ 5 \\ 4 \\ 7 \\ 9 \\ 1 \\ 2 \end{bmatrix}\right) \Longrightarrow \begin{bmatrix} 0.0002 \\ 0.7000 \\ 0.0006 \\ 0.0000 \\ 0.0047 \\ 0.0017 \\ 0.0348 \\ 0.2575 \\ 0.0001 \\ 0.0002 \end{bmatrix} \end{align*}\] <p>The ground truth one hot vector is denoted $y_i$ for sample $i$ and the output softmax distribution is denote \(\hat{y_i} = a_i\). The loss function that is traditionally used is categorical cross entropy loss. Also, assume your ground truth labels are some one hot encoded vector, with a one at the index of the true class label. Here is its form with a few simplifications:</p> \[CE(y, \hat{y}) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{j=0}^{n-1} y^j \cdot log(\hat{y_i})= -\frac{1}{m} \sum_i^{m}log(\hat{y_i})\] <p>Intuitively, for each datapoint, we are trying to maximize the log probability of the GT class, disregarding the probabilities of the other classes.</p> <h2>Softmax Temperature:</h2> <p>Now, let us modify our softmax function a little bit:</p> \[\Large{a_{i,k} := \frac{e^{\frac{z_{i,k}}{T}}}{\displaystyle\sum_{j=1}^{n-1} e^{\frac{z_{i,j}}{T}}}}\] <p>I have introduced a new hyperparameter \(T\), which is commonly called “temperature” or “softmax temperature”.</p> <p>Notice that if \(T = 1\), we simply have our old softmax expression. Firstly, convince yourself that as \(T \rightarrow \infty\), each \(a_{i,k}\) will approach \(\frac{1}{n}\). This means that as \(T\) gets larger, the softmax distribution becomes a more softer probability distribution over the classes. Here are a few examples where \(n=5\) and \(z_{i} = [1,2,3,4,5]\):</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/distillation/temperature_example-480.webp 480w,/assets/img/distillation/temperature_example-800.webp 800w,/assets/img/distillation/temperature_example-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/distillation/temperature_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As you can see, the distribution gets softer and softer peaks as we increase \(T\) and generally seems to be approaching a uniform distribution. However, relationship between the class probabilities with regards to size stays about the same. As shown above, the classes 0-4 have increasing probability from right to left, except for very high values of \(T\).</p> <h2>"Dark Knowledge":</h2> <p>Now here’s the interesting bit, assume we trained a simple feedforward classifier on MNIST (\(n = 10\)). If we sample the following image and feed it into the classifier, we get the following softmax scores. \(T\) is set to 1, so this is just with the standard softmax function.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/distillation/regular_softmax_score-480.webp 480w,/assets/img/distillation/regular_softmax_score-800.webp 800w,/assets/img/distillation/regular_softmax_score-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/distillation/regular_softmax_score.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Pretty good right? Notice that the probability for the GT class <b>4</b> is much higher than the others, so our distribution peaks very highly at a certain point. The other probabilities are very small in comparison, and are not really interpertable. Now lets turn up the temperature to some higher values of \(T\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/distillation/various_temperatures-480.webp 480w,/assets/img/distillation/various_temperatures-800.webp 800w,/assets/img/distillation/various_temperatures-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/distillation/various_temperatures.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The distribution is getting softer and softer as we increase the temperature, and we begin to see the relationship between the smaller probabilities. The ground truth class is a <b>4</b>, but the image also looks a lot like a <b>9</b>. The classifier’s softmax distribution has a high probability given to <b>9</b> for temperature values of 5 and 10 compared to the other classes. This is what we mean by “dark knowledge”. By increasing the temperature, we reveal information about what other likely classes the image can belong to. We call these probability distributions generated through softmax with temperature “soft labels”. Soft labels are a lot more meaningful than the standard one hot encoded labels since they encode information about what classes the sample resembles. This boosts its ability to classify other classes that aren’t the ground truth class.</p> <h2>Aside: Why is there a meaningful relationship between probabilities of non-ground truth classes in a softmax distribution? </h2> <p>A core reason of why knowledge distillation works is because we assume that the softmax probabilities of classes that aren’t the ground truth class are meaningful. Specifically in the knowledge distillation paper, Hinton et al. state that:</p> <p><br/> <i> “Much of the information about the learned function resides in the ratios of very small probabilities in the soft targets. For example, one version of a 2 may be given a probability of \(10^6\) of being a 3 and \(10^9\) of being a 7 whereas for another version it may be the other way around. This is valuable information that defines a rich similarity structure over the data (i. e. it says which 2’s look like 3’s and which look like 7’s)” </i></p> <p>However, I wasn’t too sure what is the reasoning behind why we can say this and had the following question: <br/></p> <p><i>Couldn’t the model learn to give a high probability to the target class for an image and meaningless assorted probabilities to the others?</i></p> <p>After struggling with this question for a while, I found a satisfying answer from asking around on <a href="https://www.reddit.com/r/learnmachinelearning/comments/t5z17u/why_is_there_a_meaningful_relationship_between/">r/learnmachinelearning </a>. Here is the short version: First, we adopt the view that the model is learning to identify some <i>high-level features</i> from a given sample, where the final softmaxed scores say “How much do this sample’s features resemble what samples from class X would look like?”. Now, in datasets there is usually overlap between samples, even samples from different classes! Therefore, due to this overlap, it is evident that each of the class probabilities should contain information about how much the sample resembles class X, rather than the non-ground-truth probabilities being meaningless noise.</p> <h2>Distillation loss:</h2> <p>Given a model trained on a dataset using the standard softmax activation (when \(T=1\)), which we call a teacher model, we want to train another model (potentially with a different architecture) called the student model.</p> <p><br/></p> <p>Denote the logits of student network as \(z_i\) and denote the logits of the teacher network as \(\tilde z_i\). The student model is trained using the following objective, with a fixed hyperparameter \(T\):</p> \[L = \alpha \cdot CE(y_i, \text{S}(z_i)) +\\ (1- \alpha) \cdot CE(\text{S}(\frac{z_i}{T}), \text{S}(\frac{\tilde z_i}{T}))\] <p>Where \(S\) is the softmax function.</p> <p><br/> <br/> As you can see, we are trying to get the student model to maximize both the probability of the ground truth class through the first term. Additionally, we are also trying to match the distribution of the student softmax to the distribution of the teacher softmax, both with temperature \(T\). We can say this because of the connections between cross entropy and KL divergence.</p> <h2>Putting it together:</h2> <p>I will go over the steps required to distill knowledge from a teacher network into a (perhaps smaller) student network.</p> <ol> <li> First, train a teacher network as you would normally, the final activation on the logits needs to be a softmax. Save the weights.</li> <li> Train a student network, another classifier with a softmax activation, using the previously explained loss </li> </ol> <p><b>Thank you for reading this post!</b></p> <p>I’ve implemented all of the concepts I’ve talked about here, you can find my code <a href="https://github.com/AditMeh/Distillation">here</a>. I plan on updating this post with my experimental results (and some particularily interesting ones around learning unseen classes) at a later date.</p>]]></content><author><name></name></author><category term="deep_learning_tricks"/><category term="computer_vision"/><summary type="html"><![CDATA[Knowledge distillation]]></summary></entry></feed>