<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Using Synthetic images as in-context samples for TabICL | Aditya Mehrotra </title> <meta name="author" content="Aditya Mehrotra"> <meta name="description" content="A quick way to fit probes using self-supervised data using TabICL"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%A6&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="adityamehrotra.ca/blog/Synthetic-TabICL/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Aditya </span> Mehrotra </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Using Synthetic images as in-context samples for TabICL</h1> <p class="post-meta"> January 13, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer_vision</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="motivation">Motivation</h1> <h2 id="in-context-learning-in-llms">In context learning in LLMs</h2> <p>In context learning is a term that is often associated with the ability for LLMs to use examples of a task to perform novel instances of that task. For example, say I want the LLM to translate a document from Alice’s writing style to Bob’s writing style. I can construct a prompt that has 10-20 samples of both of their writing styles + the author’s name, then the document’s to be translated and finally “translate the attached document’s writing style to Bob’s”. LLMs have been shown to have a remarkable ability to perform tasks with just a few examples.</p> <h2 id="tabicl">TabICL</h2> <p>Along these lines, tabular in-context learning models, or TabICL for short, is a transformer model that’s able to use in-context samples of sample-label pairs to classify novel datapoints. However unlike LLMs, these models are much more general - <strong>they operate on any arbitrary vector</strong>, not just tokenized text.</p> <p>More precisely, given \((x,y)\) pairs, where \(x \in \mathbb{R}^n\) is a sample and for a set of classes \(C \subset \mathbb{N}^{&gt;0}\), \(y \in C\) is the corresponding label, TabICL can predict label \(\hat{y}\) for an unseen sample \(\hat{x}\).</p> <p>There’s a lot more details, like how TabICL is trained and implemented, but these will maybe be in a future blog post. For now know this, <strong>TabICL has not seen a single image during it’s training</strong>.</p> <h2 id="using-tabicl-on-images">Using TabICL on images</h2> <p>Let’s try a simple test. Let’s take CIFAR10 and embed the images using a DINOV2 backbone, getting a train/test splits of 4096 dimensional embeddings. Now that we’re in the format TabICL expects, let’s take increasing percentages of our train set as ICL samples and plot how the test accuracy evolves as a function of number of ICL samples.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ICL_SSL/cifar-480.webp 480w,/assets/img/ICL_SSL/cifar-800.webp 800w,/assets/img/ICL_SSL/cifar-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ICL_SSL/cifar.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We see that we’ve managed to get a classifier that has a 90% accuracy with only 20% of the train set, along with a clear scaling of test performance with ICL samples.</p> <p>Since the backbone of TabICL is a transformer, we can also obseve a scaling law with inference time w.r.t the number of training samples</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ICL_SSL/cifar_time-480.webp 480w,/assets/img/ICL_SSL/cifar_time-800.webp 800w,/assets/img/ICL_SSL/cifar_time-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ICL_SSL/cifar_time.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>While the equation looks like a linear relationship, it’s quadratic in reality since self-attention is \(O(N^2)\). Inference becomes VERY slow with a lot of training samples, a key drawback of this method.</p> <h1 id="method">Method</h1> <p>One clear problem that arises with TabICL is that since you have quadratic scaling, there will come a point where you’ll need to construct your set of ICL samples effectively, in order to minimize runtime but maximize performance. Finding the optimal set of samples is an NP hard problem.</p> <h2 id="linear-gradient-matching">Linear Gradient Matching</h2> <p>To circumvent this, I took advantage of recent works in dataset distillation. To summarize, dataset distillation is a technique that constructs synthetic samples paired with labels, containing the <em>maximal</em> information about their class—much more than any individual sample. To do this, we draw upon a paper called “Dataset Distillation for Pre‑Trained Self‑Supervised Vision Models”, which aims to construct images that encode discriminative information about the class they belong to, which in-theory should lead to maximially informative SSL embeddings.</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ICL_SSL/linear_dd-480.webp 480w,/assets/img/ICL_SSL/linear_dd-800.webp 800w,/assets/img/ICL_SSL/linear_dd-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ICL_SSL/linear_dd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Above is their key figure. Given an SSL model \(\phi\), they construct batches of real and synthetic images, pass them through the SSL model and a linear projector $W$. The objective is to reduce the cosine distance between the gradients of the classification loss for both batches.</p> <p>Using this technique, we can construct ICL samples using perfectly synthetic data, completely avoiding picking subsets and entierly relying on synthetic data.</p> <p>Let’s benchmark this technique on the imagenette subset, a 10-class subset of imagenet. Here’s what a randomly sampled image per class looks like:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ICL_SSL/imagenette2_grid-480.webp 480w,/assets/img/ICL_SSL/imagenette2_grid-800.webp 800w,/assets/img/ICL_SSL/imagenette2_grid-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ICL_SSL/imagenette2_grid.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Now here’s what our synthetic samples per class look like:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ICL_SSL/fake_grid-480.webp 480w,/assets/img/ICL_SSL/fake_grid-800.webp 800w,/assets/img/ICL_SSL/fake_grid-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ICL_SSL/fake_grid.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="results">Results</h1> <h2 id="naive-baselines">Naive Baselines</h2> <p>Comparing this to two baselines:</p> <ul> <li>Selecting a random sample per class to be the representative</li> <li>Computing the closest sample to the centroid of each class, and using that sample as the class representative</li> </ul> <p>For the random sample baseline, here’s the distribution of test performance across 50 runs, with the red line being the mean:</p> <div class="equation"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ICL_SSL/random-480.webp 480w,/assets/img/ICL_SSL/random-800.webp 800w,/assets/img/ICL_SSL/random-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ICL_SSL/random.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Finally, the centroid baseline gets us 92%.</p> <h2 id="synthetic-samples">Synthetic samples</h2> <p>With these ICL samples, we manage to get a 96% accuracy on the test set, using only one synthetic sample per class, showing a performance gain over our baselines.</p> <h1 id="future-work">Future work</h1> <p>This was a test on one dataset with easily discriminated classes. Testing this method on more complex datasets with more classes and distribution shift should show increased benefits of using synthetic data.</p> <p>Additionally, we should also be able to plot decision boundaries (or atleast attention maps), to see how swapping train set samples with synthetic samples makes a difference.</p> </div> </article> <h2>References</h2> <div class="publications"> </div> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?8664456308d8a0b76907c75d01dd1dbf" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>